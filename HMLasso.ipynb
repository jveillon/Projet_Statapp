{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the Lasso With High Missing Rate."
      ],
      "metadata": {
        "id": "wm5MwpWa5oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to implement the lasso with high missing rate described [here](https://www.ijcai.org/proceedings/2019/0491.pdf). "
      ],
      "metadata": {
        "id": "BMpYFpPn5v0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZZL0jO1Q7HvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "pVnwGrET6JQB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMLasso"
      ],
      "metadata": {
        "id": "6FEfUyv87Lsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMLasso():\n",
        "  \"\"\"\n",
        "  Lasso regularization that performs well with high missing rate.\n",
        "\n",
        "  Implemented according to the related article 'HMLasso: Lasso with High Missing\n",
        "  Rate' by Masaaki Takada1, Hironori Fujisawa and Takeichiro Nishikawa.\n",
        "  Link to the article: https://www.ijcai.org/proceedings/2019/0491.pdf\n",
        "\n",
        "  ------------\n",
        "  Common uses: Once fitted, the HMLasso can provide linear predictions. \n",
        "  It can also be used to select variables of interest from the given data. This \n",
        "  second goal can be achieved through selection of variables whose coefficient\n",
        "  is almost (or equal to) zero.\n",
        "\n",
        "  Please note that no metric is implemented in this class for now. \n",
        "  See sklearn.metrics.mean_squared_error or like for useful metrics.\n",
        "\n",
        "  ------------\n",
        "  Parameters:\n",
        "      mu : float/int, default=1.0: the hyperparameter that control how\n",
        "      parcimonious the model shall be. The larger mu is, the greater the\n",
        "      regularization will be (hence the calculated beta_opt might \n",
        "      present more nullified coefficients). mu must be positive.\n",
        "      \n",
        "      alpha : float/int, default=1: the hyperparameter that control weights\n",
        "      importance. Be wary that setting alpha > 5 can make convergence way\n",
        "      slower, as the weights become closer and closer to 0 and as the numerical\n",
        "      solver has more and more trouble converging.\n",
        "      One may prefer setting alpha in the range [0., 3.]. Common values\n",
        "      of alpha are 0., 0.5, 1. with the latter experimentally delivering best\n",
        "      performances. alpha must be positive.\n",
        "      See source article for more.\n",
        "\n",
        "      verbose : float/int, default=1: control how much verbose\n",
        "      is displayed. Encoded values are 0, 1 and 2. If verbose > 2, there\n",
        "      will be no difference with verbose=2 display.\n",
        "  \n",
        "  ------------\n",
        "  Methods:\n",
        "      fit(self, X, y):\n",
        "        Fit the HMLasso on (X, y)\n",
        "        X, the features, must be a mean-centered numpy array of shape (n, p)\n",
        "        y, the labels, must be a vector of shape (n, 1) or (n,)\n",
        "\n",
        "        Do not return anything. However, once the fitting is done, one can\n",
        "        use 'predict' method to predict any given output using the linear model.\n",
        "      \n",
        "      predict(self, X):\n",
        "        Predict using linear model.\n",
        "        Return the predicted vector.\n",
        "  \n",
        "  ------------\n",
        "  Constants:\n",
        "      beta_opt: the estimator.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, mu=1, alpha=1, verbose=1):\n",
        "\n",
        "    assert type(mu) is int or type(mu) is float, \"mu must be a number.\"\n",
        "    assert type(alpha) is int or type(alpha) is float, \"alpha must be a number.\"\n",
        "    assert type(verbose) is int, \"verbose must be an integer.\"\n",
        "    assert mu >= 0, \"mu must be a positive number.\"\n",
        "    assert alpha >= 0, \"alpha must be a positive number.\"\n",
        "\n",
        "    self.mu = mu\n",
        "    self.alpha = alpha\n",
        "    self.verbose = verbose\n",
        "    \n",
        "    self.n = None\n",
        "    self.p = None\n",
        "    self.S_pair = None\n",
        "    self.rho_pair = None\n",
        "    self.R = None\n",
        "    self.Sigma_opt = None\n",
        "    self.beta_opt = None\n",
        "\n",
        "    self.isFirstProblemSolved = False\n",
        "    self.isSecondProblemSolved = False # Unused at the moment.\n",
        "    self.isFitted = False\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict using the linear model.\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array\n",
        "\n",
        "    Returns:\n",
        "        y : 1D numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    assert self.isFitted, \"The model has not yet been fitted.\"\n",
        "    assert X.shape[1] == self.p, f\"Given data is of dimension {X.shape[1]}. Must have dimension {self.p}).\"\n",
        "    assert not np.isnan(X).any(), \"Input contains NaN.\"\n",
        "\n",
        "    return np.dot(X, self.beta_opt)\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the HMLasso on (X, y).\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array, shape (n,p). It corresponds to the features, and\n",
        "        must be mean-centered.\n",
        "        y : 1D numpy array, shape (n,1) or (n,). It corresponds to the labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    assert type(X) == np.ndarray, \"Features are not a numpy array.\"\n",
        "    assert type(y) == np.ndarray, \"Labels are not a numpy array\"\n",
        "    assert X.shape[0] == y.shape[0], \"Features and labels shapes are not compatibles.\"\n",
        "    assert len(y.shape) == 1, \"Labels are not a vector.\"\n",
        "\n",
        "    self.n, self.p = X.shape    \n",
        "    self.__verify_centering__(X)\n",
        "    self.S_pair, self.rho_pair, self.R = self.__impute_params__(X, y)\n",
        "    self.Sigma_opt = self.__solve_first_problem__()\n",
        "\n",
        "    # It appears that, due to floating points exceptions, Sigma_opt is not always\n",
        "    # Positive semidefinite. Hence, we shall check it.\n",
        "    eigenvalues = np.linalg.eig(self.Sigma_opt)[0]\n",
        "    min_eigenvalue = min(eigenvalues)\n",
        "    if min_eigenvalue < 0:\n",
        "      print(f\"[Warning] Sigma_opt is not PSD, its minimum eigenvalue is {min_eigenvalue}. Error handled by adding {-min_eigenvalue} to each eigenvalue.\")\n",
        "      self.Sigma_opt = self.Sigma_opt - min_eigenvalue* np.eye(self.p, self.p)\n",
        "    \n",
        "    self.beta_opt = self.__solve_second_problem__()\n",
        "\n",
        "    self.isFitted = True\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"Model fitted.\")\n",
        "\n",
        "  def __verify_centering__(self, X, tolerance=1e-8):\n",
        "    for col in range(self.p):\n",
        "      current_mean = X[:, col].mean()\n",
        "      if abs(current_mean) > tolerance:\n",
        "        raise Exception(f\"Data is not centered: column {col} has mean of {current_mean}\")\n",
        "  \n",
        "  def __impute_params__(self, X, y):\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Starting...\")\n",
        "\n",
        "    Z = np.nan_to_num(X)\n",
        "    Y = (Z != 0).astype(int)\n",
        "    R = np.dot(Y.T, Y)\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] R calculated.\")\n",
        "\n",
        "    rho_pair = np.divide(np.dot(Z.T, y), R.diagonal())\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] rho_pair calculated.\")\n",
        "\n",
        "    S_pair = np.divide(np.dot(Z.T, Z), R)\n",
        "    \n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] S_pair calculated.\")\n",
        "\n",
        "    R = R / self.n\n",
        "\n",
        "    if self.alpha > 5:\n",
        "      print(\"[Warning] The hyperparameter alpha={} is large (greater than 5), which might make convergence way slower.\")\n",
        "    R = np.power(R, self.alpha)\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Parameters imputed.\")\n",
        "\n",
        "    return S_pair, rho_pair, R\n",
        "\n",
        "\n",
        "  def __solve_first_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[First Problem] Starting...\")\n",
        "\n",
        "    Sigma = cp.Variable((self.p, self.p), PSD = True) # Variable to optimize\n",
        "    obj = cp.Minimize(cp.sum_squares(cp.multiply(self.R, Sigma-self.S_pair))) # Objective to minimize\n",
        "    constraints = [Sigma >> 0] # Constraints: We want Sigma to be positive semi-definite.\n",
        "    if self.verbose > 1:\n",
        "      print(\"[First Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[First Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[First Problem] Problem solved.\")\n",
        "\n",
        "    self.isFirstProblemSolved = True\n",
        "\n",
        "    return Sigma.value\n",
        "\n",
        "  def __solve_second_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "    assert self.isFirstProblemSolved, \" First optimization problem has not been solved.\"\n",
        "    assert self.Sigma_opt is not None, \"Sigma_opt is unknown. First optimization problem might have not been solved.\"\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Second Problem] Starting...\")\n",
        "\n",
        "    beta = cp.Variable(self.p) # Variable to optimize\n",
        "    obj = cp.Minimize(0.5 * cp.quad_form(beta, self.Sigma_opt) - self.rho_pair.T @ beta + self.mu * cp.norm1(beta)) # Objective to minimize\n",
        "    constraints = [] # Constraints\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Second Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[Second Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Second Problem] Problem solved.\\n\")\n",
        "    \n",
        "    self.isSecondProblemSolved = True\n",
        "\n",
        "    return beta.value"
      ],
      "metadata": {
        "id": "sSdRL-Gm9xgh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "b6zl4SdSi4TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(HMLasso)"
      ],
      "metadata": {
        "id": "hFQnfhXSkA9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Xy(n, p, replace_rate=0.3):\n",
        "  X = 100*np.random.rand(n,p) # Generate random X\n",
        "  y = 7*X[:, 0] - 2 * X[:, 1] + 5 * X[:, 2] + 19 * X[:, 3] + 6*X[:, 4]\n",
        "  \n",
        "  indices = np.full(X.shape, False, bool)\n",
        "  mask = np.random.choice([False, True], size=X.shape, p=((1 - replace_rate), replace_rate))\n",
        "  X[mask] = np.nan\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "5mmg3-ifVCFy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  X, y = get_Xy(10000, 300, 0.4)\n",
        "\n",
        "  scaler = StandardScaler(with_std=False)\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  lasso = HMLasso(mu=100, alpha=1, verbose=2)\n",
        "  lasso.fit(X_scaled, y)\n",
        "  X_test, y_test = get_Xy(10000, 300, replace_rate=0.)\n",
        "  print(f\"error = {np.sqrt(mean_squared_error(y_test, lasso.predict(X_test)))}\\n\")\n",
        "  print(lasso.beta_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzjr9L-GqN4O",
        "outputId": "ee452ac3-8be7-4e69-b370-19cf5e5ebb1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Imputing parameters] Starting...\n",
            "[Imputing parameters] R calculated.\n",
            "[Imputing parameters] rho_pair calculated.\n",
            "[Imputing parameters] S_pair calculated.\n",
            "[Imputing parameters] Parameters imputed.\n",
            "[First Problem] Starting...\n",
            "[First Problem] Objective and constraints well-defined.\n",
            "[First Problem] Problem status: optimal.\n",
            "[First Problem] Problem solved.\n",
            "[Second Problem] Starting...\n",
            "[Second Problem] Objective and constraints well-defined.\n",
            "[Second Problem] Problem status: optimal.\n",
            "[Second Problem] Problem solved.\n",
            "\n",
            "Model fitted.\n",
            "error = 81.16303956803023\n",
            "\n",
            "[ 6.91793527e+00 -1.57233492e+00  5.08118196e+00  1.90720699e+01\n",
            "  5.83478678e+00  6.52335693e-02 -2.17276301e-22  2.03383117e-01\n",
            " -3.21218441e-02 -2.74961104e-01  2.41774393e-23 -8.22206245e-02\n",
            "  6.09315769e-22 -1.11570502e-22  9.03091810e-22  7.23923043e-22\n",
            "  9.37127520e-22 -2.46201025e-01  1.67516960e-01  6.62617522e-22\n",
            " -4.81947911e-02  1.06902052e-01 -6.54438784e-22  4.47483695e-22\n",
            "  1.66075500e-21  2.41457535e-01 -1.68067395e-21 -1.86237767e-01\n",
            " -9.47851129e-22 -1.64258997e-01 -5.02360204e-02  4.38095615e-02\n",
            " -2.81934974e-01  3.08659651e-01 -7.65179400e-02  6.04904485e-02\n",
            "  5.16063379e-22  1.00506696e-01 -1.20329701e-01 -1.23392879e-21\n",
            "  2.92625698e-01  5.90209860e-22  4.19124926e-02 -4.18823530e-02\n",
            "  1.22366443e-01  9.44609647e-22  1.49260387e-01 -2.11043529e-22\n",
            " -9.08968549e-22  9.41900205e-02  1.45390858e-01  2.56690998e-01\n",
            " -3.60005625e-02  2.67649578e-01  3.80946059e-22  3.72501216e-01\n",
            " -2.63363484e-22  8.79051641e-22  1.15228537e-21  1.40940125e-01\n",
            "  3.96600469e-22 -4.74837642e-22  1.02789354e-21  1.73062238e-21\n",
            "  7.17975569e-22  9.04360088e-02 -7.30393019e-22  4.85311338e-02\n",
            "  9.27573355e-22  1.84749355e-01  6.75197157e-22 -2.63182447e-22\n",
            "  1.04454268e-21  5.11863119e-22  1.41096109e-21  2.67727370e-01\n",
            "  3.04987094e-01  4.38209337e-02 -3.95834764e-23 -1.87933949e-22\n",
            "  3.95740529e-22 -1.17744456e-01  7.87325034e-22  3.76059801e-01\n",
            "  7.46837972e-02 -5.50732097e-22 -4.90717347e-22  9.97516662e-22\n",
            "  3.28114578e-22  2.87858408e-01  1.75472689e-01  1.66748782e-22\n",
            " -1.01414198e-21 -1.24343340e-01  3.10022354e-02  1.31763857e-02\n",
            "  2.42303189e-22 -6.53991837e-02 -3.03718198e-01  1.41776851e-02\n",
            " -3.77924989e-22 -2.58306434e-01  3.18876509e-01 -3.40775499e-02\n",
            "  3.74941903e-22  9.12359093e-02 -4.25081981e-22  1.46410383e-01\n",
            "  1.31059965e-02 -1.40904913e-03  5.02355806e-23 -2.02861555e-01\n",
            "  7.85206638e-22 -6.58090426e-22  1.98418552e-22 -1.47954488e-01\n",
            " -8.91366039e-22 -6.49798289e-22 -4.38001939e-22 -3.73089192e-01\n",
            " -1.59092659e-01  9.75731133e-02  6.22133056e-22  4.22534038e-22\n",
            " -5.46521140e-22 -2.48803038e-21  7.91666078e-02 -1.02789841e-01\n",
            "  3.69355425e-22  3.39547810e-02 -1.42719095e-01 -7.85884612e-22\n",
            " -2.44297521e-01  2.67873096e-01  3.10620065e-22  5.14435523e-02\n",
            "  1.17135735e-01 -1.34742156e-01  3.38816375e-22 -6.31083312e-01\n",
            " -1.91491427e-01  1.31264499e-01  1.25624018e-21  7.67735744e-23\n",
            "  6.18532324e-22  1.49111316e-01 -1.02496907e-21 -1.93453702e-22\n",
            " -4.56508076e-02 -8.26680212e-02 -8.25886221e-22 -8.85480334e-22\n",
            "  3.99587242e-02  2.42086797e-01 -7.79630698e-23  2.13941989e-01\n",
            "  3.06483186e-02  2.90262910e-22  4.40248029e-22 -2.14774087e-01\n",
            "  3.32480562e-01  3.61960378e-01  2.10965880e-22 -3.28836391e-01\n",
            "  1.87769129e-01  6.62358362e-03  4.16358922e-22  1.49571930e-01\n",
            "  1.00459799e-21 -1.61082771e-22 -3.28118825e-01  2.64080985e-01\n",
            "  8.79747097e-02  3.62920361e-22 -5.30857140e-01  7.09474173e-03\n",
            " -1.96728985e-21 -2.76960880e-02  3.37088965e-01 -1.12385216e-22\n",
            " -3.74456362e-22  2.22389386e-02 -1.60891800e-01 -5.02123252e-22\n",
            " -9.79708495e-22 -1.86039796e-21 -7.21157034e-22 -1.01968594e-21\n",
            "  3.77974044e-01 -1.86218808e-21 -2.57079252e-22 -4.86834424e-01\n",
            "  5.09629848e-22 -7.37321698e-02 -2.90396083e-01  2.18484443e-22\n",
            " -4.12996757e-22  2.26665230e-01  2.02286568e-01  3.05145403e-01\n",
            "  1.26739546e-01  5.35744391e-02  3.59327128e-22  1.33071274e-01\n",
            " -1.29846211e-21 -2.97620872e-01 -2.87258880e-01 -4.53116108e-01\n",
            "  7.81796426e-22 -2.93642941e-01 -5.63870582e-02  2.36562143e-01\n",
            " -3.75202992e-02 -1.72726339e-02  1.17824642e-22  2.54709335e-01\n",
            " -1.74552130e-01  4.66908358e-02  5.06650392e-02  3.74235255e-01\n",
            "  1.75130106e-01 -8.66745272e-02  8.62781842e-02 -1.70208122e-21\n",
            " -7.12372814e-03  1.40441317e-21  2.75985718e-01  2.69187344e-22\n",
            " -1.64381110e-21 -2.75229719e-01  1.06836252e-01  4.57589386e-22\n",
            "  1.93815792e-01  1.29731757e-21  1.47738394e-02 -4.18138397e-01\n",
            "  2.72871389e-01 -4.59019941e-22  3.89665720e-22 -4.84817553e-01\n",
            " -2.23584205e-01 -8.48760802e-02 -2.98068276e-01  3.74534658e-22\n",
            "  1.44452430e-21 -2.56268355e-22 -6.24647818e-22  5.57070500e-22\n",
            " -3.55843655e-01 -1.19549318e-21  1.53873734e-01  7.11860110e-02\n",
            "  8.67063731e-22 -1.73254798e-21  1.10140258e-02  2.16366138e-01\n",
            " -1.75628864e-21 -1.19940235e-01  9.43849628e-23  5.58442190e-22\n",
            "  4.35022301e-22  9.96014228e-02  1.80766166e-01  1.78478426e-21\n",
            " -5.52485989e-22 -3.59396043e-22 -1.58082524e-01  7.41540321e-22\n",
            " -6.12458859e-02 -6.91123550e-22  2.11139813e-21  8.82943364e-02\n",
            "  1.15613030e-01 -8.65860051e-23  5.79801869e-22  1.39573521e-01\n",
            " -2.42869576e-01 -1.03834431e-01 -1.96675267e-01  9.96436467e-02\n",
            "  4.84927620e-03 -3.13736867e-01  3.51708185e-01 -3.84160451e-22\n",
            " -1.83058083e-01 -3.28037861e-01  1.56190821e-01 -2.33242928e-01\n",
            "  8.33276928e-23 -2.34030847e-01  6.93373735e-02 -5.16872009e-02\n",
            " -3.42603122e-22 -9.16385041e-02 -6.74779424e-03 -1.87677979e-23\n",
            " -3.17955173e-02 -3.93058688e-01 -1.94838279e-02  1.14793997e-21]\n"
          ]
        }
      ]
    }
  ]
}