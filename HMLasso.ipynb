{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the Lasso With High Missing Rate."
      ],
      "metadata": {
        "id": "wm5MwpWa5oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to implement the lasso with high missing rate described [here](https://www.ijcai.org/proceedings/2019/0491.pdf). "
      ],
      "metadata": {
        "id": "BMpYFpPn5v0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZZL0jO1Q7HvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "pVnwGrET6JQB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(Lasso)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja1ujm1iapip",
        "outputId": "4b875cf5-bf68-4f42-e994-0f3e33ab460d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Lasso in module sklearn.linear_model._coordinate_descent:\n",
            "\n",
            "class Lasso(ElasticNet)\n",
            " |  Lasso(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
            " |  \n",
            " |  Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
            " |  \n",
            " |  The optimization objective for Lasso is::\n",
            " |  \n",
            " |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
            " |  \n",
            " |  Technically the Lasso model is optimizing the same objective function as\n",
            " |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <lasso>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  alpha : float, default=1.0\n",
            " |      Constant that multiplies the L1 term, controlling regularization\n",
            " |      strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n",
            " |  \n",
            " |      When `alpha = 0`, the objective is equivalent to ordinary least\n",
            " |      squares, solved by the :class:`LinearRegression` object. For numerical\n",
            " |      reasons, using `alpha = 0` with the `Lasso` object is not advised.\n",
            " |      Instead, you should use the :class:`LinearRegression` object.\n",
            " |  \n",
            " |  fit_intercept : bool, default=True\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to False, no intercept will be used in calculations\n",
            " |      (i.e. data is expected to be centered).\n",
            " |  \n",
            " |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
            " |      Whether to use a precomputed Gram matrix to speed up\n",
            " |      calculations. The Gram matrix can also be passed as argument.\n",
            " |      For sparse input this option is always ``False`` to preserve sparsity.\n",
            " |  \n",
            " |  copy_X : bool, default=True\n",
            " |      If ``True``, X will be copied; else, it may be overwritten.\n",
            " |  \n",
            " |  max_iter : int, default=1000\n",
            " |      The maximum number of iterations.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      The tolerance for the optimization: if the updates are\n",
            " |      smaller than ``tol``, the optimization code checks the\n",
            " |      dual gap for optimality and continues until it is smaller\n",
            " |      than ``tol``, see Notes below.\n",
            " |  \n",
            " |  warm_start : bool, default=False\n",
            " |      When set to True, reuse the solution of the previous call to fit as\n",
            " |      initialization, otherwise, just erase the previous solution.\n",
            " |      See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  positive : bool, default=False\n",
            " |      When set to ``True``, forces the coefficients to be positive.\n",
            " |  \n",
            " |  random_state : int, RandomState instance, default=None\n",
            " |      The seed of the pseudo random number generator that selects a random\n",
            " |      feature to update. Used when ``selection`` == 'random'.\n",
            " |      Pass an int for reproducible output across multiple function calls.\n",
            " |      See :term:`Glossary <random_state>`.\n",
            " |  \n",
            " |  selection : {'cyclic', 'random'}, default='cyclic'\n",
            " |      If set to 'random', a random coefficient is updated every iteration\n",
            " |      rather than looping over features sequentially by default. This\n",
            " |      (setting to 'random') often leads to significantly faster convergence\n",
            " |      especially when tol is higher than 1e-4.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
            " |      Parameter vector (w in the cost function formula).\n",
            " |  \n",
            " |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
            " |      Given param alpha, the dual gaps at the end of the optimization,\n",
            " |      same shape as each observation of y.\n",
            " |  \n",
            " |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
            " |      Readonly property derived from ``coef_``.\n",
            " |  \n",
            " |  intercept_ : float or ndarray of shape (n_targets,)\n",
            " |      Independent term in decision function.\n",
            " |  \n",
            " |  n_iter_ : int or list of int\n",
            " |      Number of iterations run by the coordinate descent solver to reach\n",
            " |      the specified tolerance.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  lars_path : Regularization path using LARS.\n",
            " |  lasso_path : Regularization path using Lasso.\n",
            " |  LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n",
            " |  LassoCV : Lasso alpha parameter by cross-validation.\n",
            " |  LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n",
            " |  sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The algorithm used to fit the model is coordinate descent.\n",
            " |  \n",
            " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
            " |  should be directly passed as a Fortran-contiguous numpy array.\n",
            " |  \n",
            " |  Regularization improves the conditioning of the problem and\n",
            " |  reduces the variance of the estimates. Larger values specify stronger\n",
            " |  regularization. Alpha corresponds to `1 / (2C)` in other linear\n",
            " |  models such as :class:`~sklearn.linear_model.LogisticRegression` or\n",
            " |  :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
            " |  assumed to be specific to the targets. Hence they must correspond in\n",
            " |  number.\n",
            " |  \n",
            " |  The precise stopping criteria based on `tol` are the following: First, check that\n",
            " |  that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n",
            " |  is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n",
            " |  If so, then additionally check whether the dual gap is smaller than `tol` times\n",
            " |  :math:`||y||_2^2 / n_{\\text{samples}}`.\n",
            " |  \n",
            " |  The target can be a 2-dimensional array, resulting in the optimization of the\n",
            " |  following objective::\n",
            " |  \n",
            " |      (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n",
            " |  \n",
            " |  where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n",
            " |  It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n",
            " |  instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n",
            " |  sparsity in the coefficients.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn import linear_model\n",
            " |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
            " |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
            " |  Lasso(alpha=0.1)\n",
            " |  >>> print(clf.coef_)\n",
            " |  [0.85 0.  ]\n",
            " |  >>> print(clf.intercept_)\n",
            " |  0.15...\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Lasso\n",
            " |      ElasticNet\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      sklearn.linear_model._base.LinearModel\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
            " |      Compute elastic net path with coordinate descent.\n",
            " |      \n",
            " |      The elastic net optimization function varies for mono and multi-outputs.\n",
            " |      \n",
            " |      For mono-output tasks it is::\n",
            " |      \n",
            " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
            " |          + alpha * l1_ratio * ||w||_1\n",
            " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
            " |      \n",
            " |      For multi-output tasks it is::\n",
            " |      \n",
            " |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
            " |          + alpha * l1_ratio * ||W||_21\n",
            " |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
            " |      \n",
            " |      Where::\n",
            " |      \n",
            " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
            " |      \n",
            " |      i.e. the sum of norm of each row.\n",
            " |      \n",
            " |      Read more in the :ref:`User Guide <elastic_net>`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
            " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
            " |          can be sparse.\n",
            " |      \n",
            " |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
            " |          Target values.\n",
            " |      \n",
            " |      l1_ratio : float, default=0.5\n",
            " |          Number between 0 and 1 passed to elastic net (scaling between\n",
            " |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
            " |      \n",
            " |      eps : float, default=1e-3\n",
            " |          Length of the path. ``eps=1e-3`` means that\n",
            " |          ``alpha_min / alpha_max = 1e-3``.\n",
            " |      \n",
            " |      n_alphas : int, default=100\n",
            " |          Number of alphas along the regularization path.\n",
            " |      \n",
            " |      alphas : ndarray, default=None\n",
            " |          List of alphas where to compute the models.\n",
            " |          If None alphas are set automatically.\n",
            " |      \n",
            " |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
            " |          Whether to use a precomputed Gram matrix to speed up\n",
            " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
            " |          matrix can also be passed as argument.\n",
            " |      \n",
            " |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
            " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
            " |          only when the Gram matrix is precomputed.\n",
            " |      \n",
            " |      copy_X : bool, default=True\n",
            " |          If ``True``, X will be copied; else, it may be overwritten.\n",
            " |      \n",
            " |      coef_init : ndarray of shape (n_features, ), default=None\n",
            " |          The initial values of the coefficients.\n",
            " |      \n",
            " |      verbose : bool or int, default=False\n",
            " |          Amount of verbosity.\n",
            " |      \n",
            " |      return_n_iter : bool, default=False\n",
            " |          Whether to return the number of iterations or not.\n",
            " |      \n",
            " |      positive : bool, default=False\n",
            " |          If set to True, forces coefficients to be positive.\n",
            " |          (Only allowed when ``y.ndim == 1``).\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          If set to False, the input validation checks are skipped (including the\n",
            " |          Gram matrix when provided). It is assumed that they are handled\n",
            " |          by the caller.\n",
            " |      \n",
            " |      **params : kwargs\n",
            " |          Keyword arguments passed to the coordinate descent solver.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      alphas : ndarray of shape (n_alphas,)\n",
            " |          The alphas along the path where models are computed.\n",
            " |      \n",
            " |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
            " |          Coefficients along the path.\n",
            " |      \n",
            " |      dual_gaps : ndarray of shape (n_alphas,)\n",
            " |          The dual gaps at the end of the optimization for each alpha.\n",
            " |      \n",
            " |      n_iters : list of int\n",
            " |          The number of iterations taken by the coordinate descent optimizer to\n",
            " |          reach the specified tolerance for each alpha.\n",
            " |          (Is returned when ``return_n_iter`` is set to True).\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
            " |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
            " |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
            " |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For an example, see\n",
            " |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
            " |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ElasticNet:\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, check_input=True)\n",
            " |      Fit model with coordinate descent.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
            " |          Data.\n",
            " |      \n",
            " |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
            " |          Target. Will be cast to X's dtype if necessary.\n",
            " |      \n",
            " |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. Internally, the `sample_weight` vector will be\n",
            " |          rescaled to sum to `n_samples`.\n",
            " |      \n",
            " |          .. versionadded:: 0.23\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Coordinate descent is an algorithm that considers each column of\n",
            " |      data at a time hence it will automatically convert the X input\n",
            " |      as a Fortran-contiguous numpy array if necessary.\n",
            " |      \n",
            " |      To avoid memory re-allocation it is advised to allocate the\n",
            " |      initial data in memory directly using that format.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from ElasticNet:\n",
            " |  \n",
            " |  sparse_coef_\n",
            " |      Sparse representation of the fitted `coef_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the coefficient of determination of the prediction.\n",
            " |      \n",
            " |      The coefficient of determination :math:`R^2` is defined as\n",
            " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
            " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
            " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always predicts\n",
            " |      the expected value of `y`, disregarding the input features, would get\n",
            " |      a :math:`R^2` score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a precomputed\n",
            " |          kernel matrix or a list of generic objects instead with shape\n",
            " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
            " |          is the number of samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True values for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
            " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
            " |      This influences the ``score`` method of all the multioutput\n",
            " |      regressors (except for\n",
            " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the linear model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
            " |          Samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array, shape (n_samples,)\n",
            " |          Returns predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMLasso"
      ],
      "metadata": {
        "id": "6FEfUyv87Lsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMLasso():\n",
        "  \"\"\"\n",
        "  Lasso regularization that performs well with high missing rate.\n",
        "\n",
        "  ------------\n",
        "  Common uses: Once fitted, the HMLasso can provide linear predictions. \n",
        "  It can also be used to select variables of interest from the given data. This \n",
        "  second goal can be achieved through selection of variables whose coefficient\n",
        "  is almost (or equal to) zero.\n",
        "\n",
        "  ------------\n",
        "  Parameters:\n",
        "      mu : float/int, default=1.0: the hyperparameter that control how\n",
        "      parcimonious the model shall be. The larger mu is, the greater the\n",
        "      regularization will be (hence the calculated beta_opt might \n",
        "      present more nullified coefficients). mu must be positive.\n",
        "\n",
        "      verbose : float/int, default=1: control how much verbose\n",
        "      is displayed. Encoded values are 0, 1 and 2. If verbose > 2, there\n",
        "      will be no difference with verbose=2 display.\n",
        "  \n",
        "  ------------\n",
        "  Methods:\n",
        "      fit(X, y):\n",
        "        Fit the HMLasso on (X, y)\n",
        "        X, the features, must be a mean-centered numpy array of shape (n, p)\n",
        "        y, the labels, must be a vector of shape (n, 1) or (n,)\n",
        "\n",
        "        Do not return anything. However, once the fitting is done, one can\n",
        "        use 'predict' method to predict any given output using the linear model.\n",
        "      \n",
        "      predict(X):\n",
        "        Predict using linear model.\n",
        "        Return the predicted vector.\n",
        "  \n",
        "  ------------\n",
        "  Constants:\n",
        "      beta_opt: the estimator.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, mu=1, verbose=1):\n",
        "\n",
        "    assert type(mu) is int or type(mu) is float, \"mu must be a number.\"\n",
        "    assert type(verbose) is int, \"verbose must be an integer.\"\n",
        "    assert mu >= 0, \"mu must be a positive number.\"\n",
        "\n",
        "    self.mu = mu\n",
        "    self.verbose = verbose\n",
        "    \n",
        "    self.n = None\n",
        "    self.p = None\n",
        "    self.S_pair = None\n",
        "    self.rho_pair = None\n",
        "    self.R = None\n",
        "    self.Sigma_opt = None\n",
        "    self.beta_opt = None\n",
        "\n",
        "    self.isFirstProblemSolved = False\n",
        "    self.isSecondProblemSolved = False\n",
        "    self.isFitted = False\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict using the linear model.\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array\n",
        "\n",
        "    Returns:\n",
        "        y : 1D numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    assert self.isFitted, \"The model has not yet been fitted.\"\n",
        "    assert X.shape == (self.n, self.p), f\"Given data is of shape {X.shape}. Must be of shape {(self.n, self.p)}\"\n",
        "\n",
        "    return np.dot(X, self.beta_opt)\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the HMLasso on (X, y).\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array, shape (n,p). It corresponds to the features, and\n",
        "        must be mean-centered.\n",
        "        y : 1D numpy array, shape (n,1) or (n,). It corresponds to the lables.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    assert type(X) == np.ndarray, \"Features are not a numpy array.\"\n",
        "    assert type(y) == np.ndarray, \"Labels are not a numpy array\"\n",
        "    assert X.shape[0] == y.shape[0], \"Features and labels shapes are not compatibles.\"\n",
        "    assert len(y.shape) == 1, \"Labels are not a vector.\"\n",
        "\n",
        "    self.n, self.p = X.shape    \n",
        "    self.__verify_centering__(X)\n",
        "    self.S_pair, self.rho_pair, self.R = self.__impute_params__(X, y)\n",
        "    self.Sigma_opt = self.__solve_first_problem__()\n",
        "\n",
        "    # It appears that, due to floating points exceptions, Sigma_opt is not always\n",
        "    # Positive semidefinite. Hence, we shall check it.\n",
        "    eigenvalues = np.linalg.eig(self.Sigma_opt)[0]\n",
        "    min_eigenvalue = min(eigenvalues)\n",
        "    if min_eigenvalue < 0:\n",
        "      print(f\"[Warning] Sigma_opt is not PSD, its minimum eigenvalue is {min_eigenvalue}. Error handled by adding {-min_eigenvalue} to each eigenvalue.\")\n",
        "      self.Sigma_opt = self.Sigma_opt - min_eigenvalue* np.eye(self.p, self.p)\n",
        "    \n",
        "    self.beta_opt = self.__solve_second_problem__()\n",
        "\n",
        "    self.isFitted = True\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"Model fitted\")\n",
        "\n",
        "  def __verify_centering__(self, X, tolerance=1e-8):\n",
        "    for col in range(X.shape[1]):\n",
        "      current_mean = X[:, col].mean()\n",
        "      if abs(current_mean) > tolerance:\n",
        "        raise Exception(f\"Data is not centered: column {col} has mean of {current_mean}\")\n",
        "  \n",
        "  def __impute_params__(self, X, y):\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Starting...\")\n",
        "\n",
        "    Z = np.nan_to_num(X)\n",
        "    Y = (Z != 0).astype(int)\n",
        "    R = np.dot(Y.T, Y)\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] R calculated.\")\n",
        "\n",
        "    rho_pair = np.divide(np.dot(Z.T, y), R.diagonal())\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] rho_pair calculated.\")\n",
        "\n",
        "    S_pair = np.divide(np.dot(Z.T, Z), R)\n",
        "    \n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] S_pair calculated.\")\n",
        "\n",
        "    R = R / self.n\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Successfully ended.\")\n",
        "\n",
        "    return S_pair, rho_pair, R\n",
        "\n",
        "\n",
        "  def __solve_first_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "\n",
        "    Sigma = cp.Variable((self.p, self.p), PSD = True) # Variable to optimize\n",
        "    obj = cp.Minimize(cp.sum_squares(cp.multiply(self.R, Sigma-self.S_pair))) # Objective to minimize\n",
        "    constraints = [Sigma >> 0] # Constraints: We want Sigma to be semi definite positive.\n",
        "    if self.verbose > 1:\n",
        "      print(\"[First Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[First Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[First Problem] Problem solved.\")\n",
        "\n",
        "    self.isFirstProblemSolved = True\n",
        "\n",
        "    return Sigma.value\n",
        "\n",
        "  def __solve_second_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "    assert self.isFirstProblemSolved, \" First optimization problem has not been solved.\"\n",
        "    assert self.Sigma_opt is not None, \"Sigma_opt is unknown. First optimization problem might have not been solved.\"\n",
        "    \n",
        "    beta = cp.Variable(self.p) # Variable to optimize\n",
        "    obj = cp.Minimize(0.5 * cp.quad_form(beta, self.Sigma_opt) - self.rho_pair.T @ beta + self.mu * cp.norm1(beta)) # Objective to minimize\n",
        "    constraints = [] # Constraints\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Second Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[Second Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Second Problem] Problem solved.\\n\")\n",
        "    \n",
        "    self.isFirstProblemSolved = True\n",
        "\n",
        "    return beta.value"
      ],
      "metadata": {
        "id": "sSdRL-Gm9xgh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "def get_Xy(n, p, replace_rate=0.3):\n",
        "  X = 100*np.random.rand(n,p) # Generate random X\n",
        "  y = 7*X[:, 0] - 2 * X[:, 1] + 5 * X[:, 2] + 19 * X[:, 3] + 6*X[:, 4]\n",
        "  \n",
        "  indices = np.full(X.shape, False, bool)\n",
        "  mask = np.random.choice([False, True], size=X.shape, p=((1 - replace_rate), replace_rate))\n",
        "  X[mask] = np.nan\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X, y = get_Xy(15000, 1500, 0.4)\n",
        "\n",
        "scaler = StandardScaler(with_std=False)\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "5mmg3-ifVCFy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = HMLasso(mu=1, verbose=2)\n",
        "lasso.fit(X_scaled, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTsjf5XN8Kxe",
        "outputId": "7b5db1d6-636a-4069-80c0-77b82f88e1af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Imputing parameters] Starting...\n",
            "[Imputing parameters] Z calculated.\n",
            "[Imputing parameters] Y calculated.\n",
            "[Imputing parameters] R calculated.\n",
            "[Imputing parameters] rho_pair calculated.\n",
            "[Imputing parameters] S_pair calculated.\n",
            "[Imputing parameters] Successfully ended.\n",
            "[First Problem] Objective and constraints well-defined.\n",
            "[First Problem] Problem status: optimal.\n",
            "[First Problem] Problem solved.\n",
            "[Second Problem] Objective and constraints well-defined.\n",
            "[Second Problem] Problem status: optimal.\n",
            "[Second Problem] Problem solved.\n",
            "\n",
            "Model fitted\n"
          ]
        }
      ]
    }
  ]
}