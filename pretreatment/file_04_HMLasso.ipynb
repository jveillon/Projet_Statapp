{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the Lasso With High Missing Rate."
      ],
      "metadata": {
        "id": "wm5MwpWa5oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to implement the lasso with high missing rate described [here](https://www.ijcai.org/proceedings/2019/0491.pdf). "
      ],
      "metadata": {
        "id": "BMpYFpPn5v0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZZL0jO1Q7HvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "pVnwGrET6JQB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMLasso"
      ],
      "metadata": {
        "id": "6FEfUyv87Lsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMLasso():\n",
        "  \"\"\"\n",
        "  Lasso regularization that performs well with high missing rate.\n",
        "\n",
        "  Implemented according to the related article 'HMLasso: Lasso with High Missing\n",
        "  Rate' by Masaaki Takada1, Hironori Fujisawa and Takeichiro Nishikawa.\n",
        "  Link to the article: https://www.ijcai.org/proceedings/2019/0491.pdf\n",
        "\n",
        "  ------------\n",
        "  Common uses: Once fitted, the HMLasso can provide linear predictions. \n",
        "  It can also be used to select variables of interest from the given data. This \n",
        "  second goal can be achieved through selection of variables whose coefficient\n",
        "  is almost (or equal to) zero.\n",
        "\n",
        "  Please note that no metric is implemented in this class for now. \n",
        "  See sklearn.metrics.mean_squared_error or like for useful metrics.\n",
        "\n",
        "  ------------\n",
        "  Parameters:\n",
        "      mu : float/int, default=1.0: the hyperparameter that control how\n",
        "      parcimonious the model shall be. The larger mu is, the greater the\n",
        "      regularization will be (hence the calculated beta_opt might \n",
        "      present more nullified coefficients). mu must be positive.\n",
        "      \n",
        "      alpha : float/int, default=1: the hyperparameter that control weights\n",
        "      importance. Be wary that setting alpha > 5 can make convergence way\n",
        "      slower, as the weights become closer and closer to 0 and as the numerical\n",
        "      solver has more and more trouble converging.\n",
        "      One may prefer setting alpha in the range [0., 3.]. Common values\n",
        "      of alpha are 0., 0.5, 1. with the latter experimentally delivering best\n",
        "      performances. alpha must be positive.\n",
        "      See source article for more.\n",
        "\n",
        "      verbose : float/int, default=1: control how much verbose\n",
        "      is displayed. Encoded values are 0, 1 and 2. If verbose > 2, there\n",
        "      will be no difference with verbose=2 display.\n",
        "  \n",
        "  ------------\n",
        "  Methods:\n",
        "      fit(self, X, y):\n",
        "        Fit the HMLasso on (X, y)\n",
        "        X, the features, must be a mean-centered numpy array of shape (n, p)\n",
        "        y, the labels, must be a vector of shape (n, 1) or (n,)\n",
        "\n",
        "        Do not return anything. However, once the fitting is done, one can\n",
        "        use 'predict' method to predict any given output using the linear model.\n",
        "      \n",
        "      predict(self, X):\n",
        "        Predict using linear model.\n",
        "        Return the predicted vector.\n",
        "  \n",
        "  ------------\n",
        "  Constants:\n",
        "      beta_opt: the estimator.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, mu=1, alpha=1, verbose=1):\n",
        "\n",
        "    assert type(mu) is int or type(mu) is float, \"mu must be a number.\"\n",
        "    assert type(alpha) is int or type(alpha) is float, \"alpha must be a number.\"\n",
        "    assert type(verbose) is int, \"verbose must be an integer.\"\n",
        "    assert mu >= 0, \"mu must be a positive number.\"\n",
        "    assert alpha >= 0, \"alpha must be a positive number.\"\n",
        "\n",
        "    self.mu = mu\n",
        "    self.alpha = alpha\n",
        "    self.verbose = verbose\n",
        "    \n",
        "    self.n = None\n",
        "    self.p = None\n",
        "    self.S_pair = None\n",
        "    self.rho_pair = None\n",
        "    self.R = None\n",
        "    self.Sigma_opt = None\n",
        "    self.beta_opt = None\n",
        "\n",
        "    self.isFirstProblemSolved = False\n",
        "    self.isSecondProblemSolved = False # Unused at the moment.\n",
        "    self.isFitted = False\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict using the linear model.\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array\n",
        "\n",
        "    Returns:\n",
        "        y : 1D numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    assert self.isFitted, \"The model has not yet been fitted.\"\n",
        "    assert X.shape[1] == self.p, f\"Given data is of dimension {X.shape[1]}. Must have dimension {self.p}).\"\n",
        "    assert not np.isnan(X).any(), \"Input contains NaN.\"\n",
        "\n",
        "    return np.dot(X, self.beta_opt)\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the HMLasso on (X, y).\n",
        "\n",
        "    ------------\n",
        "    Parameters:\n",
        "        X : 2D numpy array, shape (n,p). It corresponds to the features, and\n",
        "        must be mean-centered.\n",
        "        y : 1D numpy array, shape (n,1) or (n,). It corresponds to the labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    assert type(X) == np.ndarray, \"Features are not a numpy array.\"\n",
        "    assert type(y) == np.ndarray, \"Labels are not a numpy array\"\n",
        "    assert X.shape[0] == y.shape[0], \"Features and labels shapes are not compatibles.\"\n",
        "    assert len(y.shape) == 1, \"Labels are not a vector.\"\n",
        "\n",
        "    self.n, self.p = X.shape    \n",
        "    self.__verify_centering__(X)\n",
        "    self.S_pair, self.rho_pair, self.R = self.__impute_params__(X, y)\n",
        "    self.Sigma_opt = self.__solve_first_problem__()\n",
        "\n",
        "    # It appears that, due to floating points exceptions, Sigma_opt is not always\n",
        "    # Positive semidefinite. Hence, we shall check it.\n",
        "    eigenvalues = np.linalg.eig(self.Sigma_opt)[0]\n",
        "    min_eigenvalue = min(eigenvalues)\n",
        "    if min_eigenvalue < 0:\n",
        "      print(f\"[Warning] Sigma_opt is not PSD, its minimum eigenvalue is {min_eigenvalue}. Error handled by adding {-min_eigenvalue} to each eigenvalue.\")\n",
        "      self.Sigma_opt = self.Sigma_opt - min_eigenvalue * np.eye(self.p, self.p)\n",
        "    \n",
        "    self.beta_opt = self.__solve_second_problem__()\n",
        "\n",
        "    self.isFitted = True\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"Model fitted.\")\n",
        "\n",
        "  def __verify_centering__(self, X, tolerance=1e-8):\n",
        "    for col in range(self.p):\n",
        "      current_mean = X[:, col].mean()\n",
        "      if abs(current_mean) > tolerance:\n",
        "        raise Exception(f\"Data is not centered: column {col} has mean of {current_mean}\")\n",
        "  \n",
        "  def __impute_params__(self, X, y):\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Starting...\")\n",
        "\n",
        "    Z = np.nan_to_num(X)\n",
        "    Y = (Z != 0).astype(int)\n",
        "    R = np.dot(Y.T, Y)\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] R calculated.\")\n",
        "\n",
        "    rho_pair = np.divide(np.dot(Z.T, y), R.diagonal(), out=np.zeros((self.p,)), where=(R.diagonal()!=0))\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] rho_pair calculated.\")\n",
        "\n",
        "    S_pair = np.divide(np.dot(Z.T, Z), R, out=np.zeros((self.p, self.p)), where=(R!=0))\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Imputing parameters] S_pair calculated.\")\n",
        "\n",
        "    R = R / self.n\n",
        "\n",
        "    if self.alpha > 5:\n",
        "      print(\"[Warning] The hyperparameter alpha={} is large (greater than 5), which might make convergence way slower.\")\n",
        "    R = np.power(R, self.alpha)\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Imputing parameters] Parameters imputed.\")\n",
        "\n",
        "    return S_pair, rho_pair, R\n",
        "\n",
        "\n",
        "  def __solve_first_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[First Problem] Starting...\")\n",
        "\n",
        "    Sigma = cp.Variable((self.p, self.p), PSD = True) # Variable to optimize\n",
        "    obj = cp.Minimize(cp.sum_squares(cp.multiply(self.R, Sigma-self.S_pair))) # Objective to minimize\n",
        "    constraints = [Sigma >> 0] # Constraints: We want Sigma to be positive semi-definite.\n",
        "    if self.verbose > 1:\n",
        "      print(\"[First Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[First Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[First Problem] Problem solved.\")\n",
        "\n",
        "    self.isFirstProblemSolved = True\n",
        "\n",
        "    return Sigma.value\n",
        "\n",
        "  def __solve_second_problem__(self):\n",
        "    \n",
        "    assert self.S_pair is not None, \"Pairwise covariance matrix of features is not determined.\"\n",
        "    assert self.rho_pair is not None, \"Pairwise covariance vector of features and labels is not determined.\"\n",
        "    assert self.R is not None, \"Weights are not determined.\"\n",
        "    assert self.isFirstProblemSolved, \" First optimization problem has not been solved.\"\n",
        "    assert self.Sigma_opt is not None, \"Sigma_opt is unknown. First optimization problem might have not been solved.\"\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Second Problem] Starting...\")\n",
        "\n",
        "    beta = cp.Variable(self.p) # Variable to optimize\n",
        "    obj = cp.Minimize(0.5 * cp.quad_form(beta, self.Sigma_opt) - self.rho_pair.T @ beta + self.mu * cp.norm1(beta)) # Objective to minimize\n",
        "    constraints = [] # Constraints\n",
        "    if self.verbose > 1:\n",
        "      print(\"[Second Problem] Objective and constraints well-defined.\")\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    prob = cp.Problem(obj, constraints)\n",
        "    prob.solve()\n",
        "    if self.verbose > 1:\n",
        "      print(f\"[Second Problem] Problem status: {prob.status}.\")\n",
        "    if self.verbose > 0:\n",
        "      print(\"[Second Problem] Problem solved.\\n\")\n",
        "    \n",
        "    self.isSecondProblemSolved = True\n",
        "\n",
        "    return beta.value"
      ],
      "metadata": {
        "id": "sSdRL-Gm9xgh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "b6zl4SdSi4TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Xy(n, p, replace_rate=0.3):\n",
        "  X = 100*np.random.rand(n,p) # Generate random X\n",
        "  y = 7*X[:, 0] - 2 * X[:, 1] + 5 * X[:, 2] + 19 * X[:, 3] + 6*X[:, 4]\n",
        "  \n",
        "  indices = np.full(X.shape, False, bool)\n",
        "  mask = np.random.choice([False, True], size=X.shape, p=((1 - replace_rate), replace_rate))\n",
        "  X[mask] = np.nan\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "5mmg3-ifVCFy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  X, y = get_Xy(10000, 20, 0.4)\n",
        "\n",
        "  scaler = StandardScaler(with_std=False)\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  lasso = HMLasso(mu=100, alpha=1, verbose=2)\n",
        "  lasso.fit(X_scaled, y)\n",
        "  X_test, y_test = get_Xy(10000, 20, replace_rate=0.)\n",
        "  print(f\"error = {np.sqrt(mean_squared_error(y_test, lasso.predict(X_test)))}\\n\")\n",
        "  print(lasso.beta_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzjr9L-GqN4O",
        "outputId": "f25c078f-3ca4-4f44-874d-1174b8bcda37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Imputing parameters] Starting...\n",
            "[Imputing parameters] R calculated.\n",
            "[Imputing parameters] rho_pair calculated.\n",
            "[Imputing parameters] S_pair calculated.\n",
            "[Imputing parameters] Parameters imputed.\n",
            "[First Problem] Starting...\n",
            "[First Problem] Objective and constraints well-defined.\n",
            "[First Problem] Problem status: optimal.\n",
            "[First Problem] Problem solved.\n",
            "[Second Problem] Starting...\n",
            "[Second Problem] Objective and constraints well-defined.\n",
            "[Second Problem] Problem status: optimal.\n",
            "[Second Problem] Problem solved.\n",
            "\n",
            "Model fitted.\n",
            "error = 20.783485676418394\n",
            "\n",
            "[ 6.92524510e+00 -1.83085597e+00  5.07955149e+00  1.89969772e+01\n",
            "  5.84850541e+00  2.44939746e-01 -1.73239684e-01  1.40939784e-02\n",
            "  2.45775015e-01 -2.37982828e-01  9.78327019e-23 -2.09841088e-01\n",
            " -1.03225534e-22 -6.53642454e-02  5.07929645e-22  2.95445073e-01\n",
            "  4.20127847e-02  5.21161400e-22  2.21063980e-02 -3.69975834e-23]\n"
          ]
        }
      ]
    }
  ]
}