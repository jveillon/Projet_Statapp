\documentclass[]{article}

%packages
\usepackage{graphicx} % Pour les images et les graphiques
\usepackage{pythonhighlight} % Pour le code Python
\usepackage{amsfonts} % Pour les maths
\usepackage{amsmath} % Pour les maths
\usepackage[super]{nth} % Pour écrire 1st, 2nd, 3rd, 54th, etc.
\usepackage{geometry} % Pour changer les marges

% UTILISER LA LIGNE CI-DESSOUS POUR CHANGER LES MARGES
\newgeometry{vmargin={20mm}, hmargin={30mm,30mm}}   % set the margins

%new commands
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\card}{\mathrm{card}}
\DeclareMathOperator*{\R}{\mathbb{R}}
\DeclareMathOperator*{\towrite}{\textbf{REMPLIR}}
\DeclareMathOperator*{\addref}{\textbf{AJOUTER REFERENCE}}


%opening
\title{Applied Statistics Project - On the predictability of life course}
\author{
	VEILLON, Juliette\\
	\texttt{first1.last1@xxxxx.com}
	\and
	ANDRU, Kilian\\
	\texttt{first2.last2@xxxxx.com}
	\and
	LACOUR, Xavier\\
	\texttt{first2.last2@xxxxx.com}
	\and
	MASSIN, Keryann\\
	\texttt{first2.last2@xxxxx.com}
}

\begin{document}
\newcommand{\sklearn}{\textbf{scikit-learn}}
\maketitle
\begin{abstract}
	Based on socioeconomic and genetic data, is it possible to predict one's health across time? The question has been discussed in many previous research without satisfying answer. This paper explains how we approached the problem. The data used for our purposes are the ones from the Health and Retirement Study led by the RAND Corporation. We first explain how me chose the global health index, using Stochastic Neighbors Embedding paradigm, before applying a specific kind of Lasso Regression and further machine learning algorithms.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}
The work presented in this document has been realized in the framework of the \textbf{Applied Statistics Project} (projet Statapp) by the joint participation of Juliette Veillon, Kilian Andru, Xavier Lacour and Keryann Massin. It is dedicated to the question of the \textbf{predictability of life course}, a sociological question approached from a data science perspective.\\

At the era of overly present information, machine learning reveals links and hidden relationships between variables that used to be hard to tackle. Data Science techniques spread among experts of all fields, and social sciences see major breakthroughs thanks to artificial intelligence. The question of the predictability of life course comes in this context as a challenging one: there is an obvious relationship between age, body-fat ratio and health for instance, but how could one exploit these links and other available information for prediction purpose? For instance, is it possible to forecast one's overall health for the upcoming year based on the socioeconomic and genetic data at our disposal? That is the problem we wish to address with this STATAPP project.\\

Thanks to our supervisor M. Tropf, we have had access to the Heath and Retirement Study (HRS) led by the RAND Corporation, that consists in interviews of seniors in the United States followed over several years (28 years at most). $\towrite$

\section{Presentation and cleaning of the database}
\subsection{Presentation}
Our project is based on multiple databases, which all have the same source: the Health and Retirement Study (HRS). The HRS is sponsored by the National Institute on Aging (grant number NIA \textit{U01AG009740}) and is conducted by the University of Michigan $\addref$ and focuses on Americans over $50$. More than $20,000$ are included in the survey. The original goals of the survey were to help facilitate research and to guide policymakers in their decisions. 

The first dataset is about participants’ lives data (its formal name is RAND HRS Longitudinal File 2018 (V2) $\addref$). It contains fourteen waves of survey for the moment. At each new wave, people from previous waves must respond again (as the data is longitudinal), and new people representative of the American society are also added. Are considered in the data the respondents themselves, their spouses, and their households. The data here revolves around the socio-economic situation of the respondents but also their health. It is outlined by more than $10,000$ variables. 

The two other datasets correspond to the genetic ones. They are similar and just differ as one contains people with European ancestry while the other people with African ancestry. The contained genetic data are polygenic scores (PGSs). Their goal is to encapsulate genetic facts that are linked to a phenotypic trait to try to find the latter. Here, the genetic facts in question are Single Nucleotide Polymorphisms (SNPs). They correspond to DNA sequences where only one particular gene is different from the others in a part of the population wider than $1$\%. To obtain a PGS, a weighted sum is made with the SNPs linked to the phenotype that one wishes to estimate. The weights are obtained with a meta-analysis done on genome-wide association studies (GWAS) which try to see the link between phenotypes and genes (but not with single genes like with the SNPs).
\subsection{Initial merges}
To conduct our project, we first need to merge the databases. As the people who responded to the genetics survey are also included in the socio-economic one, we only need to add the first to the second by the individual identifier \pyth!HHIDPN!. However, this variable does not exist in the genetics databases, so we had to compute it by concatenating the Household Identifier \pyth!HHID! with the Person Number \pyth!PN!. Then we concatenated the two genetics bases and merged the resulting base with the socio-economic one. As the two genetic bases only differed by the individuals represented (ones with European ancestry and ones with African ancestry) but not by any of the $86$ variables of interest, we decided to create an \nth{87} variable \pyth!Section_A_or_E! that takes \pyth!A! and \pyth!E! as modalities and indicates the ancestry of individuals, in order not to lose any information.\\
\textbf{Notebook de référence : file\_0\_merge\_Section\_A\_and\_E}
\subsection{Removal of unusable variables}
At this point, our database gathers $n=42233$ individuals and $p=15104$ variables. With a size of about $1.15$ gigabytes and quadruple in RAM,our laptops could not handle the whole dataset and perform appropriate analysis. Plus, considering that the dataset presents a high missing values rate and that $p\approx n$, machine learning algorithms would not be able to converge in the current state of our data. We need to clean it.

\subsubsection{Removal of Spouses' related variables}
The subject of the variables always fall into one of the three categories 'Respondent, Spouse, Household'. Because Spouses' related variables amounted to around $7200$, we decided to drop them all, considering the fact that $95$\% of spouses were also interviewed as respondents.
At the end of this operation, our database is of size $(n=42233, p=7939)$.

\subsubsection{Removal of columns with a missing rate too high}
The quickest way to efficiently reduce the size of our database is to directly drop useless columns. Our data mainly corresponding to answers to interviews, some columns present a missing rate really high. We calculated for each wave the number of columns for a given missing rate, and got the graph $\ref{distribution_of_columns_wrt_missing_values_ratio}$ in appendix. The genetic data are available for around $35.97$\% of the respondents, so we decided to drop all variables that presented a missing rate higher than $65$\ref{LassoStandardFirstForm}%.\\
A precision is needed: the vast majority of the columns in our dataset are in fact timed realizations of one global variable. Take \pyth!R1MSTAT! for example. It appears that \pyth!R1MSTAT, R2MSTAT, ..., R14MSTAT! are the responses of individuals to the same question, say \pyth!RwMSTAT!, asked at different waves. Hence, it makes sense for these 14 variables to not be treated separately: if we delete \pyth!R12MSTAT!, we shall delete \pyth!R1MSTAT, R2MSTAT,! etc. If we keep one column, we keep all of them.\\
There are $156$ variables like \pyth!RwMSTAT! with an average missing rate of more than $65$\%. It corresponds to $1577$ columns in our original dataset, and we dropped them all at once. This single operation drastically reduced the size of the database from $(n = 42233, p = 7939)$ to $(n = 42233, p = 6131)$.\\

\textit{Remark.} During the process of cleaning the dataset, we came across a variable named \pyth!FILEVER! that were constant (\pyth!FILEVER = 'U'!) for all individuals. We decided to drop it.

\subsubsection{Removal of health variables}
The whole point of our project is to predict a global health index. As explained further on, we used the health variables to construct our index. To remove endogeneity issues, we decided to drop all health variables (even though we used only $27$ of the $1983$ available to create our index) in the main database, and save them in a separate database. At the end of this operation, our database is of size $(n = 42233, p = 4147)$. No obvious columns removal could be made for now.

\section{Definition of the objective}
As explained in the introduction, we want to see whether it is possible to forecast someone's health with the socioeconomic and genetic data we have about him at our disposal. We formally need a \textit{Global Health Index}, that could summarize the whole of health-related information into one real number. Our goal will then be to attempt to predict it. This section is dedicated to the creation of this index.\\

We carefully selected a few quantitative health-variables, from his age and body mass index to the number of overnight stays at hospital or in nursing home he went through over a year. We also took into account qualitative variables indicating whether the respondent had had hypertension, cancer, diabete, heart disease, etc. The problem was to summarize $27$ health-variables into one Global Health Index.\\

The scientific literature points out the benefits of Principal Component Analysis (PCA) in the search of such an index. Therefore our first attempts were to use PCA to summarize the $27$ variables into one. However, the index created this way did not present the expected consistency: one's index looked random across time, and the index was awkwardly positively correlated with each health-variable, independently of the actual effect of the variable on one's health.\\

Hence, we looked for another dimensional reduction algorithm. The t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has caught our attention, and we decided to apply it on our health data.

\subsection{t-SNE algorithm: presentation}
The PCA is designed to reduce a dataset of high dimension $d$ into a smaller space of dimension $d'<\!<d$ by finding the \textit{principal components}, that is the $d'$ maximum variance directions, and project the original data in the smaller space. Globally, this is an linear algebraic oriented approach.\\
On the other hand, the t-distributed Stochastic Neighbor Embedding tries to find similarities in the data using a non-linear probabilistic approach. The overall idea consists in assigning to the original data a probability measure $P$ and another one $Q$ to the reduced data, then trying to alter the reduced data such that the Kullbach-Leibler divergence of the distribution $P$ and $Q$ is minimal.
\subsection{Formal details}
Formally, considering $(x_1, x_2,\dots x_n) \in \R^{n\times d}$ the original high-dimensional dataset and the reduced dataset $(y_1,y_2,\dots, y_n)\in \R^{n\times d'}$, we compute the similarity of $x_j$ to $x_i$ by the conditional probability $p_{j\mid i}$ that $x_i$ would pick $x_j$ as its neighbor under a Gaussian distribution centered at $x_i$, where:
\begin{equation}
	p_{i\mid i} = 0\quad \text{and}\quad
	p_{j\mid i} = \frac{\exp(-\|x_i - x_j\|^2/2\sigma_i^2)}{\sum_{k\ne i}\exp(-\|x_i - x_k\|^2/2\sigma_k^2)}, \quad \forall i\ne j
\end{equation}
with $\sigma_i$ the bandwidth of the Gaussian kernels.\\
Once $p_{ij} := (p_{i\mid j} + p_{j\mid i})/(2n)$ defined, we also define a probability measure $Q$ on the reduced dataset following a Student t-distribution:
\begin{equation}
	q_{ij} = \frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_k \sum_{\ell \ne k}	(1+\|y_k-y_{\ell}\|^2)^{-1}}
\end{equation}
The locations of points $y_i$ are determined by minimizing the Kullbach-Leibler divergence of $P$ from $Q$, i.e minimizing
\begin{equation}
	KL(P\|Q) = \sum_{i\ne j}p_{ij} \ln\left(\frac{p_{ij}}{q_{ij}}\right)
\end{equation}
with respect to $y_1,y_2,\dots, y_n$, by a gradient descent.
\subsection{Performances}
The main objective of this algorithm is to reflect the similarities in the original data. We directly applied the \pyth!sklearn.manifold.TSNE! implementation on our health data and checked its consistency. The results are detailed below and in appendix.\\

The index (denoted \textit{GHI} for \textit{Global Health Index} in the following) we obtained was calculated separately on each wave, only using the health information available at the current wave. Without surprise, the GHI empirically follows a Student t-distribution, and globally takes its values in the interval $[-150, 150]$.\\

As we will have to estimate an index based on a testing sample of our data, the first question to be answered is the following: does the GHI of an individual change when we do not take into account the whole database in the process? For instance, if one's GHI were equal to $50$ when the index has been fitted on the whole database, would it still be equal to $50$ if the index were this time fitted on a sample of the data? The figure $\ref{RMSE_according_to_sample_size}$ in appendix demonstrates the robustness of t-SNE algorithm in generating an index not too much data-dependent: the root mean squared error observed for samples of $10,000$ individuals amounts to about $27$, for an index that varies between $-150$ and $150$.\\

In addition, figures \ref{relationships_between_tSNE_GHI_and_multiple_factors} and \ref{tSNE_GHI_by_age} show that the GHI is consistent: highest values of the GHI refer to very fragile health, while lowest ones refer to healthiest individuals. Indeed, an older age will generally correspond to a more fragile health, and so will a high number of overnight hospital stays for instance.
\section{First machine learning procedures to format the database}
At this point, the dataset we are working on still presents a few thousands variables. Considering that it approaches the number of individuals (that is, $p \approx n$), machine learning algorithms will have struggle converging, or would converge to unsatisfying solutions. Hence, we shall reduce the number of such variables by other means than deletion (every possible removals having already been previously realized).\\
According to the literature, among the many possible techniques to reduce dimensionality, the Lasso Regression is the suitable one. The idea is to train a Lasso Regression on $(X, y)$, where $X$ are the available data and $y$ is the global health index, to come up with an estimator. The Lasso estimator has the property to shutdown to zero its coordinates when the associated variable does not present a prediction power strong enough. This technique has the advantage of being quick to train and to apply. \textit{A priori}, the estimator we would get from this approach will not be really accurate at predicting, but it will not matter as we are only interested in its coefficients' nullity.\\
The library \sklearn\, in Python would allow us to reach our goal through the \pyth!sklearn.linear_model.Lasso! class. However this is bound to fail in our specific case, because $X$ presents many missing values, as discussed in sections before. Hence, we have to adapt the method. For the record, we will present the three methods we thought of, the two first ones having failed.

\subsection{Imputation of missing data}
To still apply the lasso as discussed before, we could try imputing the missing data. Some imputers are already implemented in \sklearn, such as the \pyth!SimpleImputer(strategy="mean")! that naturally imputes missing values in one column with the empiric mean of this column. The problem with this approach is that it does not take into account the inter-variable relationships in imputation at all. To overcome this issue, we would rather use the \pyth!IterativeImputer()!, which relies on the correlations and relationships between columns to impute missing values. Its inner algorithm calculates at each iteration a new imputed database $\tilde{X}_t$ according to specific constraints, and tries to minimize the difference $\Delta_t = \tilde{X}_{t} - \tilde{X}_{t-1}$. When this difference is small enough, the algorithm stops and the solution $\tilde{X}_t$ is returned. This resulting database would present no missing values, and imputed data would be a lot more likely than what the \pyth!SimpleImputer()! would obtain. But despite all our attempts, the algorithm did not converge in any way. This is due to the fact that $X$ is too big and the computer had trouble processing. This idea failed.

\subsection{Convex conditioned Lasso}
Instead of looking for an application in two steps of the Lasso (first imputing the data, then fitting the Lasso), another idea is to directly modify the Lasso optimization program so that it handles missing values by itself, without too heavy imputations. The state-of-art algorithm for that problem is the \textit{CoCoLasso} (which stands for \textit{Convex Conditioned Lasso}). This algorithm badly performs when missing rates are too high, which is our case. We then cannot use it in its first form. However, further researches helped us discover a very similar estimator, that we shall now present.

\subsection{Lasso with High Missing Rate}
The idea of directly applying the Lasso still being interesting, our final approach is also based on it. According to the paper REF, it is possible to edit the CoCoLasso to make it more appropriate for dataset with high missing rate. Let us dive into the mathematical details.\\

\subsubsection{Why is CoCoLasso not fitted for high missing rate}
Consider a linear regression model $y = X\beta + \varepsilon$, where $X\in \R^{n\times p}$ , $y\in \R^n$ are the data, $\varepsilon\in \mathbb{R}^n$ a noise and $\beta\in \R^p$ the estimator we wish to determine. We assume that $y$ and each column of $X$ are centered.\\
If $X$ does not contain missing values, Lasso is applicable, and its optimization program is:
\begin{equation}
	\label{LassoStandardFirstForm}
	\hat{\beta} = \argmin\limits_{\beta} \frac{1}{2n}\|y - X\beta\|_2^2 + \mu \|\beta\|_1
\end{equation}
where $\mu > 0$ is a regularization parameter, $\|\cdot\|_1$ is the $\ell_1$ norm and $\|\cdot\|_2$ is the $\ell_2$ norm.
By the way, the presence of $\ell_1$ norm is responsible for the solution $\hat{\beta}$ to have many null coordinates (the property we are interested in).\\
The equation \eqref{LassoStandardFirstForm} can be equivalently rewritten as following:
\begin{equation}
	\label{LassoStandardSecondForm}
	\hat{\beta} = \argmin\limits_{\beta} \frac{1}{2}\beta^T S\beta - \rho^T\beta + \mu \|\beta\|_1
\end{equation}
where $S=\frac{1}{n}X^TX$ is the sample covariance matrix of $X$ and $\rho = \frac{1}{n}X^Ty$ is the sample covariance vector of $X$ and $y$.\\
In presence of missing values, This means that we can just estimate $S$ and $\rho$ without having to impute the whole database, and this is in fact a much easier task to achieve. Indeed,  we can construct unbiased estimators of $S$ and $\rho$ using the pairwise covariance, that is, $S^{\text{pair}} = (S^{\text{pair}}_{jk})$ and $\rho^{\text{pair}} = (\rho^{\text{pair}}_{j})$ where:
\begin{equation}
	S^{\text{pair}}_{jk} = \frac{1}{n_{jk}} \sum_{i \in I_{jk}} X_{ij}X_{jk} \quad \text{and}\quad \rho^{\text{pair}}_{j} = \frac{1}{n_{jj}} \sum_{i \in I_{jj}} X_{ij}y_i
\end{equation}
for $I_{jk} = \{i\mid \text{$X_{ij}$ and $X_{jk}$ are observed}\}$ and $n_{jk} = \card I_{jk}$. Thus, we can relace $S$ and $\rho$ by $S^{\text{pair}}$ and $\rho^{\text{pair}}$ in \eqref{LassoStandardSecondForm}.\\
The major problem here is that $S^{\text{pair}}$ may \textit{not} be positive semidefinite (PSD). This would lead \eqref{LassoStandardSecondForm} to have no real solution and is therefore a fatal issue. The easiest way to resolve this problem is to replace $S^{\text{pair}}$ by $\tilde{\Sigma}$, defined as:
\begin{equation}
	\tilde{\Sigma} = \argmin_{\Sigma \succeq 0} \|\Sigma - S^{\text{pair}}\|_{\text{max}}
\end{equation}
where $\|A\|_{\text{max}} := \max_{i,j} |A_{ij}|$.
Denoting $\Sigma \succeq 0$ a PSD matrix $\Sigma$, the optimization problem becomes:
\begin{equation}
	\label{CoCoLassoProgram}
	\left\{
	\begin{array}{ll}
	\tilde{\Sigma} &= \argmin\limits_{\Sigma \succeq 0} \|\Sigma - S^{\text{pair}}\|_{\text{max}}\\
	\hat{\beta} &= \argmin\limits_{\beta} \frac{1}{2}\beta^T \tilde{\Sigma}\beta - {\rho^{\text{pair}}}^T\beta + \mu \|\beta\|_1\end{array}\right.
\end{equation}
The system \eqref{CoCoLassoProgram} is actually the optimization program solved by the CoCoLasso discussed in the previous section.\\

As stated before, a high missing rate can deteriorate estimations of the covariance matrix in CoCoLasso: if some pairwise observation numbers $n_{jk}$ are very small, then the corresponding pairwise covariances $S^{\text{pair}}_{jk}$ are quite statistically unreliable. As a result, other estimator elements can highly deviate from the corresponding elements in $S^{\text{pair}}$, even if their variables have few missing values. The core issue is that CoCoLasso does not account for the differences in reliability of the pairwise covariance. The next section describes how to overcome this problem.

\subsubsection{Advantages of HMLasso over CoCoLasso}
To fully understand the advantages oh HMLasso (High Missing rate Lasso) over CoCoLasso, let us first describe the program it solves.\\
Let $Z$ be the mean imputed data of $X$. That is:
\begin{equation}
	Z_{jk} = \left\{
	\begin{array}{ll}
		X_{jk} & \mbox{if $X_{jk}$ is observed}\\
		0 & \mbox{otherwise}
	\end{array}\right.
\end{equation}
We then compute the mean imputed covariance matrix, where each coefficient is weighted 
\begin{equation}
	S^{\text{imp}}_{jk} = \frac{n_{jk}}{n}\cdot S^{\text{pair}}_{jk}
\end{equation}
Thus, $S^{\text{imp}} = R \odot S^{\text{pair}}$ where $\odot$ denotes the element-wise product and $R = (n_{jk}/n)$ is the matrix of weights. This allows us to take into account the difference of reliability of each coefficients of $S^{\text{pair}}$\\
We can then state the HMLasso optimization program:
\begin{equation}
	\label{HMLassoProgram}
	\left\{
	\begin{array}{ll}
		\tilde{\Sigma} &= \argmin\limits_{\Sigma \succeq 0} \|W\odot(\Sigma - S^{\text{pair}})\|_2^2\\
		\hat{\beta} &= \argmin\limits_{\beta} \frac{1}{2}\beta^T \tilde{\Sigma}\beta - {\rho^{\text{pair}}}^T\beta + \mu \|\beta\|_1
	\end{array}\right.
\end{equation}
with $W$ the weight matrix of coefficients $W_{jk} = R_{jk}^{\alpha}$ for a fixed constant $\alpha \geq 0$.\\

This program had the $\|\cdot \|_{\text{max}}$ norm replaced by the $\ell_2$ norm and a weight matrix ($W$) added in the first optimization problem to account for the reliability of each $S^{\text{pair}}_{jk}$. As a result, this implementation is apt to efficiently deal with high missing rates.
\subsubsection{Implementation in Python}
A research in the literature did not provide us with any already implemented instance of the HMLasso in Python, so we decided to create our own.\\
The \pyth!cvxpy! library provided the solver, and we were able to create a class \pyth!HMLasso(mu = 1, alpha = 1)! that fits $(X, y)$ and returns an estimator \pyth!beta_opt! useful to drop variables whose coefficient in \pyth!beta_opt! are almost $0$. Below is an example of what the algorithm does and a sample of its source code:
\begin{python}
	from HMLasso import HMLasso
	lasso = HMLasso(mu = 10, alpha = 1) # set values of mu and alpha
	lasso.fit(X, y)
	estimator = lasso.beta_opt
	y_pred = lasso.predict(X_test) # predict X_test using lasso
\end{python}
And the source code:
\begin{python}
	class HMLasso():
		def __init__(self, mu=1, alpha=1):
		...
		
		def fit(self, X, y):
			self.__verify_centering__(X, y)
			S_pair, rho_pair, R = self.__impute_params__(X, y)
			self.Sigma_opt = self.__solve_first_problem__()
			self.beta_opt = self.__solve_second_problem__()
			
		def predict(self, X):
			return np.dot(X, self.beta_opt)
		
		...
\end{python}
The implementation presented a lot of difficulties, one of them regarding the fact that the underlying solver was not able to assert the positive semidefinitiveness of \pyth!Sigma_opt! due to its large size, even though the matrix was indeed PSD. As this assertion was an integral part of the resolution process, getting around this check presented many challenges, not to mention that each correction attempt required 30 minutes of simulation. In the final version of the \pyth!HMLasso()!, the user is invited to handle such errors when they happen by manually setting \pyth!ERRORS_HANDLING =  "ignore"!, meaning \textit{'ignore PSD check'}.

\cleardoublepage
\appendix
\section{Appendix}
\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.75]{distribution_of_columns_wrt_missing_values_ratio.png}
	\caption{Distribution of columns with respect to missing values ratio}
	\label{distribution_of_columns_wrt_missing_values_ratio}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{tSNE_GHI_RMSE_according_to_sample_size.png}
\caption{RMSE according to the sample size}
\label{RMSE_according_to_sample_size}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{relationships_between_tSNE_GHI_and_multiple_factors.png}
\caption{Relationships between the global health index and multiple factors}
\label{relationships_between_tSNE_GHI_and_multiple_factors}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{tSNE_GHI_by_age.png}
\caption{Relationship between the global health index and age}
\label{tSNE_GHI_by_age}
\end{figure}
Figure \ref{distribution_of_columns_wrt_missing_values_ratio} is meant to be interpreted like this: among the variables collected from the interviews in wave 2, $40$\% of them present more than $45$\% missing values.\\

Figure \ref{RMSE_according_to_sample_size} plots the root mean-squared-error observed between the actual index and the index determined on a sample of the data, size of which being informed on the $x$ axis.\\

Figure \ref{relationships_between_tSNE_GHI_and_multiple_factors} describes how the GHI is changed when a certain factor is present. The category $0$ stands for \textit{no such factor}, category $1$ for \textit{there is this factor}, and other categories express a higher level of the current factor. For instance, \pyth!DIAB = 0! means that the individual does not present diabete. \pyth!CANCR, BACK, HEART, LUNG! stand for the presence of cancer, of pain or health problems in the back, the heart, or the lung. Overall, what we see was expected: when some one has arthrose problems (\pyth!ARTHR!), his general health is expected to be lower, hence his GHI to be higher. The unique variable that behave differently is \pyth!DRINK!, that describes how much one drinks alcohol. But the deviation is marginal.\\

Figure \ref{tSNE_GHI_by_age} highlights the expected fact that someone's health is better when he is young.






\newpage
\begin{thebibliography}{9}
	\bibitem{texbook}
	Donald E. Knuth (1986) \emph{The \TeX{} Book}, Addison-Wesley Professional.
	\bibitem{lamport94}
	Leslie Lamport (1994) \emph{\LaTeX: a document preparation system}, Addison
	Wesley, Massachusetts, 2nd ed.
\end{thebibliography}

\end{document}
