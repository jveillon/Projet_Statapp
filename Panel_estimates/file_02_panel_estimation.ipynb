{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0307ef",
   "metadata": {},
   "source": [
    "Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747ca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # To standardize the data\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb11d",
   "metadata": {},
   "source": [
    "Import of the HMLasso function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07cd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path \"C:/Users/Kilian/Desktop/ENSAE/STATAPP\" to run the cell\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Kilian/Desktop/ENSAE/STATAPP/Projet_Statapp/pretreatment')\n",
    "\n",
    "import file_04_HMLasso as hml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72f6ac",
   "metadata": {},
   "source": [
    "## Data downloading and separation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca111d",
   "metadata": {},
   "source": [
    "Dataset containing the types of each column from data_03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16325af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HHIDPN</td>\n",
       "      <td>Cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HHID</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PN</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  Type\n",
       "0  HHIDPN  Cont\n",
       "1    HHID  Char\n",
       "2      PN  Char"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_types = pd.read_csv(\"data_03_columns_types.csv\", index_col=0)\n",
    "columns_types.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbe86a",
   "metadata": {},
   "source": [
    "Downloading the data with social and genetic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015166c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5468\\3952530018.py:1: DtypeWarning: Columns (2684) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data_03.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aec591",
   "metadata": {},
   "source": [
    "The column \"genetic_Section_A_or_E\" have mixed types, so we change its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965a817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = np.where(data['genetic_Section_A_or_E'] == 'E', 1, np.where(data['genetic_Section_A_or_E'] == 'A', 0, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f5d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genetic_Section_A_or_E\"] = temporary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b4e72",
   "metadata": {},
   "source": [
    "Now we add the health index created by t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6a30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_GHI = pd.read_csv(\"data_tSNE_GHI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fa4f3",
   "metadata": {},
   "source": [
    "We merge the t-SNE health index to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c121e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(tSNE_GHI, how ='left', on ='HHIDPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf31a8d",
   "metadata": {},
   "source": [
    "The final outcome to predict is tSNE_GHI14, so we only keep individuals who were interviewed during the last wave (14th wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ebbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bis = data[data['tSNE_GHI14'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ead7a",
   "metadata": {},
   "source": [
    "Number of individuals present in every waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7abd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSNE_GHI[~tSNE_GHI.isnull().any(axis=1)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04aaf5",
   "metadata": {},
   "source": [
    "We select the outcome tSNE_GHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_bis[[\"HHIDPN\"]+[\"tSNE_GHI\" + str(i) for i in range (1,15)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563e1a",
   "metadata": {},
   "source": [
    "We drop the previous health index GHIw from the data, which won't be used as outcome.\n",
    "(list_columns_GHI contains the names of GHIw columns).\n",
    "\n",
    "We drop the outcome to create the matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_bis.drop([\"GHI\" + str(i) for i in range (1,15)], axis = 1)\n",
    "X.drop([\"tSNE_GHI\" + str(i) for i in range (1,15)], axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228abcf",
   "metadata": {},
   "source": [
    "Now we split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bebb3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test, test_size=0.5, random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7c060",
   "metadata": {},
   "source": [
    "Smaller sets while coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51094fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test, nb_train, nb_valid = len(X_test.index)//10, len(X_train.index)//10, len(X_valid.index)//10\n",
    "X_test, Y_test = X_test.iloc[:nb_test], Y_test.iloc[:nb_test]\n",
    "X_train, Y_train = X_train.iloc[:nb_train], Y_train.iloc[:nb_train]\n",
    "X_valid, Y_valid = X_valid.iloc[:nb_valid], Y_valid.iloc[:nb_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ddedcf",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46496dbf",
   "metadata": {},
   "source": [
    "The objective here is to make a dataset where we observe if each variable exists at each wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cde8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_temporal_variables (X_train,add_tSNE_GHIw):   \n",
    "    temporal_variables = {}\n",
    "    waves_columns = [col for col in X_train.columns if \"genetic_\" not in col and col[1] in \"123456789\"]\n",
    "    for col in waves_columns:\n",
    "      char = col[0] # R or H\n",
    "      if col[2] in \"01234\":\n",
    "        wave = col[1:3]\n",
    "        suffix = col[3:]\n",
    "      else:\n",
    "        wave = col[1]\n",
    "        suffix = col[2:]\n",
    "      variable = char + 'w' + suffix\n",
    "\n",
    "      if variable not in temporal_variables.keys():\n",
    "        temporal_variables[variable] = np.zeros((14), dtype=bool)\n",
    "\n",
    "      temporal_variables[variable][int(wave)-1] = True\n",
    "\n",
    "    temporal_variables = pd.DataFrame(temporal_variables)\n",
    "\n",
    "    # We manually add \"tSNE_GHIw\":\n",
    "    if add_tSNE_GHIw:\n",
    "        temporal_variables[\"tSNE_GHIw\"] = np.ones((14), dtype=bool)\n",
    "        waves_columns += [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "        \n",
    "    return (temporal_variables,waves_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8869d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeless data\n",
    "def timeless_variables(X_train,waves_columns):\n",
    "    non_waves_columns = [col for col in X_train.columns if col not in waves_columns]\n",
    "    To_remove = [\"HHIDPN\",\"PN\",\"HHID\",\"RAHHIDPN\"]+[\"INW\"+str(i+1) for i in range (14)]\n",
    "    for x in To_remove:\n",
    "        non_waves_columns.remove(x)\n",
    "    return non_waves_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b3d71",
   "metadata": {},
   "source": [
    "We put the explaining variables by wave in a list of dataset Intemporal variables are put in each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3fbfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def list_wave(X_train, reduced):\n",
    "    (temporal_variables , waves_columns) = dataset_temporal_variables(X_train,True)\n",
    "    non_waves_columns = timeless_variables(X_train,waves_columns)\n",
    "    \n",
    "    #Reduce number of variables to code\n",
    "    if reduced:\n",
    "        temporal_variables_2 = temporal_variables.iloc[:,[i for i in range(1,15)]+[-i for i in range(1,5)]]\n",
    "        non_waves_columns_2 = random.choices(non_waves_columns,k=5)\n",
    "\n",
    "        liste = [] \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables_2.T[i].index[temporal_variables_2.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Add the intemporal variables only to the last wave, to avoid duplicated labels issues\n",
    "            if i == 13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns_2])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    #All the variables\n",
    "    else:\n",
    "        liste = []    # len = 14 \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables.T[i].index[temporal_variables.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Intemporal variables only in the first wave, to avoid duplicated labels issues\n",
    "            if i ==  13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    return (liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea850340",
   "metadata": {},
   "source": [
    "### HMLasso selection\n",
    "\n",
    "We start to initialize with a first lasso on the first wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6c57b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lasso(liste, Y_train, mu, limit):\n",
    "    \n",
    "    print(\"wave\",1)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    #Prepare data\n",
    "    X_train1 = liste[0].drop(\"HHIDPN\",axis=1)\n",
    "    Y_train1 = Y_train.iloc[:,1]\n",
    "    Y_train1.dropna(inplace =True)\n",
    "    Y_train1 = Y_train1.values\n",
    "    Y_train1 = (Y_train1 - np.mean(Y_train1))/np.std(Y_train1)\n",
    "    \n",
    "    #Standardize X_train\n",
    "    X_train1 = scaler.fit_transform(X_train1)\n",
    "    \n",
    "    #HMLasso\n",
    "    lasso = hml.HMLasso(mu)\n",
    "    lasso.fit(X_train1, Y_train1)\n",
    "    \n",
    "    #Selection of variables\n",
    "    coefficients = np.abs(lasso.beta_opt.copy())\n",
    "\n",
    "    var_to_keep = coefficients < 10**(limit)\n",
    "    var_to_keep = np.insert(var_to_keep,0,False)\n",
    "    \n",
    "    entire_data = liste[0]\n",
    "    selected = entire_data[entire_data.columns[~var_to_keep]]\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9394e1",
   "metadata": {},
   "source": [
    "function to impute missing data created when merging by mean but without touching Na values already there before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab72eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_management(df1, df2, index):\n",
    "    \n",
    "    merged = df1.merge(df2, how='outer', on = index)\n",
    "    \n",
    "    df1_index = df1.set_index(index)\n",
    "    df2_index = df2.set_index(index)\n",
    "    \n",
    "    merged = merged.fillna(merged.mean())\n",
    "    merged = merged.set_index(index)\n",
    "    \n",
    "    df1_index = df1_index.fillna(\"NaN\")\n",
    "    merged.update(df1_index)\n",
    "    \n",
    "    df2_index = df2_index.fillna(\"NaN\")\n",
    "    merged.update(df2_index)\n",
    "    \n",
    "    merged = merged.replace(\"NaN\",np.nan)\n",
    "    \n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a387d69",
   "metadata": {},
   "source": [
    "Function to select variables by HMLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "539e1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso_selection(X_train, Y_train, mu, limit, reduced):\n",
    "    \n",
    "    liste = list_wave(X_train, reduced)\n",
    "    \n",
    "    print(\"HMLasso\")\n",
    "    \n",
    "    selected = initialize_lasso(liste, Y_train, mu, limit)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    for i in range (1,14) :\n",
    "    \n",
    "        print(\"wave\",i+1)\n",
    "\n",
    "        var_to_select = Na_management(selected, liste[i], \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = Y_train.iloc[:,[0,i+1]]\n",
    "        X_Y_train = var_to_select.merge(Y_train_i, how = 'left', on = \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = X_Y_train[f\"tSNE_GHI{i+1}\"]\n",
    "        X_train_i = X_Y_train.drop([f\"tSNE_GHI{i+1}\",\"HHIDPN\"], axis =1)\n",
    "\n",
    "        Y_train_i = Y_train_i.fillna(Y_train_i.mean())\n",
    "        Y_train_i = Y_train_i.values\n",
    "        Y_train_i = (Y_train_i - np.mean(Y_train_i))/np.std(Y_train_i)\n",
    "\n",
    "        X_train_i = scaler.fit_transform(X_train_i)\n",
    "\n",
    "        lasso = hml.HMLasso(mu)\n",
    "        lasso.fit(X_train_i, Y_train_i)\n",
    "\n",
    "        coefficients = np.abs(lasso.beta_opt.copy())\n",
    "\n",
    "        var_to_keep = coefficients < 10**(limit)\n",
    "        var_to_keep = np.insert(var_to_keep,0,False)\n",
    "\n",
    "        entire_data = var_to_select\n",
    "        selected = entire_data[entire_data.columns[~var_to_keep]] \n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418df308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMLasso\n",
      "wave 1\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is (-9.66308241883724e-06+0j). Error handled by adding (9.66308241883724e-06-0j) to each eigenvalue.\n",
      "wave 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:985: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:990: RuntimeWarning: invalid value encountered in true_divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:1020: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is (-4.97460253166036e-05+0j). Error handled by adding (4.97460253166036e-05-0j) to each eigenvalue.\n",
      "wave 3\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is (-4.072910322486161e-05+0j). Error handled by adding (4.072910322486161e-05-0j) to each eigenvalue.\n",
      "wave 4\n"
     ]
    }
   ],
   "source": [
    "selected =Lasso_selection(X_train, Y_train, mu = 200, limit = -14, reduced=False)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605b486",
   "metadata": {},
   "source": [
    "### Within estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cba15",
   "metadata": {},
   "source": [
    "There two types of missing values, the \"one-time\" missing values when someone didn't awnser a question during the interview or so and the missing values when someone wasn't interviewed at all during a wave.\n",
    "\n",
    "\n",
    "For the first type, we impute those missing values with the mean of the column (Nan).\n",
    "(Possibility to work on another imputation method).\n",
    "\n",
    "Then for the individuals who weren't interviewed during a wave, we replace the missing value with the temporal mean of the variable over time (NanNan)\n",
    "\n",
    "\n",
    "Finally, we compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2ba7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_data_within (selected):\n",
    "    \n",
    "    #For the \"one-time\" missing values imputation by mean\n",
    "    X_train_within = selected.fillna(selected.mean())\n",
    "    \n",
    "    ###For people who weren't interviewed\n",
    "    # We start by adding the INWw columns to know if the individual was interviewed during the wave w\n",
    "    X_train_within = X_train_within.merge(X_train[[\"HHIDPN\"]+[\"INW\"+str(i) for i in range(1,15)]], how =\"left\", on=\"HHIDPN\")\n",
    "    \n",
    "    #We recover the missing values for people who weren't interviewed during the wave w\n",
    "    X_train_within = recover_missing(X_train_within)\n",
    "    \n",
    "    # Creation of the data set for within regression.\n",
    "    #X_train_within = data_set_within (X_train_within)\n",
    "    temporal_variables_within = temporal_variables(X_train_within, False)[0]\n",
    "    \n",
    "    return X_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feface31",
   "metadata": {},
   "source": [
    "This function return a dataset containing only variables concerned by the wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464508b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "def get_wave(data, wave):\n",
    "  \"\"\"\n",
    "  This function returns a smaller dataset summarizing all data for the given wave.\n",
    "\n",
    "  Note that it also returns columns that are not relative to any wave (for instance, 'HHIDPN')\n",
    "  \"\"\"\n",
    "\n",
    "  assert wave in range(1, 15)\n",
    "\n",
    "  regex = re.compile(\"[0-9]+\")\n",
    "  wave_columns = [col for col in data.columns if (len(regex.findall(col)) == 0 or regex.findall(col)[0] == str(wave))]\n",
    "  wave_data = data[wave_columns]\n",
    "\n",
    "  return wave_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf1ef0",
   "metadata": {},
   "source": [
    "Function to recover the missing values for people who weren't interviewed during the wave w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd5b9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_missing(X_train_within):\n",
    "### Problem here\n",
    "    X_train_within_index = X_train_within.set_index(\"HHIDPN\")\n",
    "    wave_1 = get_wave(X_train_within_index,1)\n",
    "    wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
    "    wave_1[\"INW1\"].fillna(0, inplace =True)\n",
    "    Tempo = wave_1\n",
    "\n",
    "    for i in range(2,15):\n",
    "        wave_i = get_wave(X_train_within_index, i)\n",
    "        wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
    "        wave_i[\"INWw\".replace('w', str(i))].fillna(0, inplace =True)\n",
    "        Tempo = Tempo.merge(wave_i, how= \"left\", on = \"HHIDPN\")\n",
    "    \n",
    "    X_train_within = Tempo.reset_index()\n",
    "\n",
    "    return X_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de283800",
   "metadata": {},
   "source": [
    "Get a dataframe to know which variables are in X_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4691c",
   "metadata": {},
   "source": [
    "Function to compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)\n",
    "\n",
    "It creates columns containing the temporal mean of a temporal variables and then replaces the Nan values (when people weren't interviewed) by this mean. Finally it creates the dataset  X_vague_ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d188ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set_within (X_train_within):\n",
    "    temporal_variables_within = dataset_temporal_variables(X_train_within, False)[0]\n",
    "    for col in temporal_variables_within.columns:\n",
    "        index_wave = temporal_variables_within.index[temporal_variables_within[col]==1].tolist()\n",
    "        names_waves = [col.replace('w', str(i+1)) for i in index_wave]\n",
    "        # (~X_train_within[names_waves].isna()).sum(axis=1) = number of non missing values\n",
    "        X_train_within[col+\"_MEAN\"] = X_train_within[names_waves].sum(axis=1)/(~X_train_within[names_waves].isna()).sum(axis=1)\n",
    "        for x in names_waves:\n",
    "            # Imputing the missing values by the temporal mean\n",
    "            new_col = X_train_within[x].fillna(X_train_within[col+\"_MEAN\"], inplace =True)\n",
    "            X_train_within[x] = new_col            \n",
    "            #Creating the new data for within regression X_vague\n",
    "            X_train_within[x] = X_train_within[x] - X_train_within[col+\"_MEAN\"]\n",
    "    return X_train_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_within = selected.fillna(selected.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ac4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding the INWw columns to know if the individual was interviewed during the wave w\n",
    "X_train_within = X_train_within.merge(X_train[[\"HHIDPN\"]+[\"INW\"+str(i) for i in range(1,15)]], how =\"left\", on=\"HHIDPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea63b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_within_index = X_train_within.set_index(\"HHIDPN\")\n",
    "wave_1 = get_wave(X_train_within_index,1)\n",
    "wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
    "wave_1[\"INW1\"].fillna(0)\n",
    "Tempo = wave_1\n",
    "\n",
    "for i in range(2,15):\n",
    "    wave_i = get_wave(X_train_within_index, i)\n",
    "    wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
    "    wave_i[\"INWw\".replace('w', str(i))].fillna(0)\n",
    "    Tempo = Tempo.merge(wave_i, how= \"left\", on = \"HHIDPN\")\n",
    "    \n",
    "X_train_within = Tempo.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_variables_within = dataset_temporal_variables(X_train_within, False)[0]\n",
    "\n",
    "for col in temporal_variables_within.columns:\n",
    "    index_wave = temporal_variables_within.index[temporal_variables_within[col]==1].tolist()\n",
    "    names_waves = [col.replace('w', str(i+1)) for i in index_wave]\n",
    "    # (~X_train_within[names_waves].isna()).sum(axis=1) = number of non missing values\n",
    "    X_train_within[col+\"_MEAN\"] = X_train_within[names_waves].sum(axis=1)/(~X_train_within[names_waves].isna()).sum(axis=1)\n",
    "    for x in names_waves:\n",
    "        # Imputing the missing values by the temporal mean\n",
    "        new_col = X_train_within[x].fillna(X_train_within[col+\"_MEAN\"])\n",
    "        X_train_within[x] = new_col\n",
    "        #Creating the new data for within regression X_vague\n",
    "        X_train_within[x] = X_train_within[x] - X_train_within[col+\"_MEAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f94fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still Nan values in intemporal variables\n",
    "X_train_within = X_train_within.fillna(X_train_within.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488b723",
   "metadata": {},
   "source": [
    "Now we do the same thing to Y_train but no need to impute the Nan values since the only outcome is tSNE_GHI14 (no Nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cf9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_within = Y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ada198",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_GHI = [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "\n",
    "Y_train_within[\"tSNE_GHIw_MEAN\"] = Y_train_within[tSNE_GHI].sum(axis=1)/(~Y_train_within[tSNE_GHI].isna()).sum(axis=1)\n",
    "Y_train_within[\"tSNE_GHI14_within\"] = Y_train_within[\"tSNE_GHI14\"] - Y_train_within[\"tSNE_GHIw_MEAN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95850bfa",
   "metadata": {},
   "source": [
    "We can now proceed to the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regression = X_train_within.merge(Y_train_within[[\"HHIDPN\",\"tSNE_GHI14_within\"]], on = \"HHIDPN\")\n",
    "Y_regression = data_regression[\"tSNE_GHI14_within\"]\n",
    "list_to_drop = [\"HHIDPN\",\"tSNE_GHI14_within\"]+[\"INW\"+str(i) for i in range(1,15)]+[col+\"_MEAN\" for col in temporal_variables_within.columns]\n",
    "X_regression = data_regression.drop(list_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "modeleReg=LinearRegression()\n",
    "\n",
    "modeleReg.fit(X_regression,Y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modeleReg.intercept_)\n",
    "print(modeleReg.coef_)\n",
    "\n",
    "#calcul du R²\n",
    "modeleReg.score(X_regression,Y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e86e68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHIDPN</th>\n",
       "      <th>tSNE_GHI1</th>\n",
       "      <th>tSNE_GHI2</th>\n",
       "      <th>tSNE_GHI3</th>\n",
       "      <th>tSNE_GHI4</th>\n",
       "      <th>tSNE_GHI5</th>\n",
       "      <th>tSNE_GHI6</th>\n",
       "      <th>tSNE_GHI7</th>\n",
       "      <th>tSNE_GHI8</th>\n",
       "      <th>tSNE_GHI9</th>\n",
       "      <th>tSNE_GHI10</th>\n",
       "      <th>tSNE_GHI11</th>\n",
       "      <th>tSNE_GHI12</th>\n",
       "      <th>tSNE_GHI13</th>\n",
       "      <th>tSNE_GHI14</th>\n",
       "      <th>tSNE_GHIw_MEAN</th>\n",
       "      <th>tSNE_GHI14_within</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35141</th>\n",
       "      <td>540304010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.804930</td>\n",
       "      <td>61.886147</td>\n",
       "      <td>63.345539</td>\n",
       "      <td>-1.459392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30248</th>\n",
       "      <td>502218020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.852474</td>\n",
       "      <td>37.104267</td>\n",
       "      <td>56.103394</td>\n",
       "      <td>50.725037</td>\n",
       "      <td>-6.405303</td>\n",
       "      <td>-39.898094</td>\n",
       "      <td>6.425584</td>\n",
       "      <td>-15.314579</td>\n",
       "      <td>17.199097</td>\n",
       "      <td>-32.513676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40563</th>\n",
       "      <td>907690020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.819390</td>\n",
       "      <td>48.003716</td>\n",
       "      <td>-14.311967</td>\n",
       "      <td>-45.567310</td>\n",
       "      <td>27.308905</td>\n",
       "      <td>11.650547</td>\n",
       "      <td>15.658358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6954</th>\n",
       "      <td>47057010</td>\n",
       "      <td>-57.655020</td>\n",
       "      <td>24.923943</td>\n",
       "      <td>-19.287235</td>\n",
       "      <td>-11.867963</td>\n",
       "      <td>-45.765656</td>\n",
       "      <td>-51.708080</td>\n",
       "      <td>66.945540</td>\n",
       "      <td>19.890480</td>\n",
       "      <td>-44.744183</td>\n",
       "      <td>-76.391785</td>\n",
       "      <td>-15.420312</td>\n",
       "      <td>-26.248714</td>\n",
       "      <td>-20.455055</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>-18.546666</td>\n",
       "      <td>16.677376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26950</th>\n",
       "      <td>213031010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-53.555614</td>\n",
       "      <td>-24.401985</td>\n",
       "      <td>-92.002350</td>\n",
       "      <td>-27.655800</td>\n",
       "      <td>59.701550</td>\n",
       "      <td>66.647790</td>\n",
       "      <td>99.778190</td>\n",
       "      <td>3.915255</td>\n",
       "      <td>19.751297</td>\n",
       "      <td>6.718432</td>\n",
       "      <td>-20.618025</td>\n",
       "      <td>3.479885</td>\n",
       "      <td>-24.097910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32542</th>\n",
       "      <td>525847020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-55.090855</td>\n",
       "      <td>-20.056421</td>\n",
       "      <td>-20.870111</td>\n",
       "      <td>57.827724</td>\n",
       "      <td>65.454980</td>\n",
       "      <td>5.453063</td>\n",
       "      <td>60.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28487</th>\n",
       "      <td>500892010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.402885</td>\n",
       "      <td>-101.593440</td>\n",
       "      <td>-18.303556</td>\n",
       "      <td>-67.452095</td>\n",
       "      <td>-77.100980</td>\n",
       "      <td>-65.834885</td>\n",
       "      <td>-66.255850</td>\n",
       "      <td>-64.370340</td>\n",
       "      <td>-59.664254</td>\n",
       "      <td>-4.706086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15978</th>\n",
       "      <td>177715010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.898155</td>\n",
       "      <td>-41.924114</td>\n",
       "      <td>-79.426710</td>\n",
       "      <td>-41.448692</td>\n",
       "      <td>-55.587166</td>\n",
       "      <td>-58.608147</td>\n",
       "      <td>-52.567307</td>\n",
       "      <td>-45.682487</td>\n",
       "      <td>95.338830</td>\n",
       "      <td>-56.426400</td>\n",
       "      <td>94.130300</td>\n",
       "      <td>-21.118522</td>\n",
       "      <td>115.248822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>24400020</td>\n",
       "      <td>6.360288</td>\n",
       "      <td>60.418080</td>\n",
       "      <td>111.114470</td>\n",
       "      <td>23.114595</td>\n",
       "      <td>0.542709</td>\n",
       "      <td>11.005449</td>\n",
       "      <td>7.788031</td>\n",
       "      <td>70.938190</td>\n",
       "      <td>-12.596005</td>\n",
       "      <td>74.012405</td>\n",
       "      <td>81.418304</td>\n",
       "      <td>58.977135</td>\n",
       "      <td>70.751150</td>\n",
       "      <td>20.058348</td>\n",
       "      <td>41.707368</td>\n",
       "      <td>-21.649020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32231</th>\n",
       "      <td>524711010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.579679</td>\n",
       "      <td>8.713304</td>\n",
       "      <td>7.781396</td>\n",
       "      <td>15.033057</td>\n",
       "      <td>68.194290</td>\n",
       "      <td>22.260345</td>\n",
       "      <td>45.933945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1371 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          HHIDPN  tSNE_GHI1  tSNE_GHI2   tSNE_GHI3  tSNE_GHI4  tSNE_GHI5  \\\n",
       "35141  540304010        NaN        NaN         NaN        NaN        NaN   \n",
       "30248  502218020        NaN        NaN         NaN        NaN        NaN   \n",
       "40563  907690020        NaN        NaN         NaN        NaN        NaN   \n",
       "6954    47057010 -57.655020  24.923943  -19.287235 -11.867963 -45.765656   \n",
       "26950  213031010        NaN        NaN         NaN -53.555614 -24.401985   \n",
       "...          ...        ...        ...         ...        ...        ...   \n",
       "32542  525847020        NaN        NaN         NaN        NaN        NaN   \n",
       "28487  500892010        NaN        NaN         NaN        NaN        NaN   \n",
       "15978  177715010        NaN        NaN         NaN   9.898155 -41.924114   \n",
       "3200    24400020   6.360288  60.418080  111.114470  23.114595   0.542709   \n",
       "32231  524711010        NaN        NaN         NaN        NaN        NaN   \n",
       "\n",
       "       tSNE_GHI6  tSNE_GHI7   tSNE_GHI8  tSNE_GHI9  tSNE_GHI10  tSNE_GHI11  \\\n",
       "35141        NaN        NaN         NaN        NaN         NaN         NaN   \n",
       "30248        NaN  48.852474   37.104267  56.103394   50.725037   -6.405303   \n",
       "40563        NaN        NaN         NaN        NaN   42.819390   48.003716   \n",
       "6954  -51.708080  66.945540   19.890480 -44.744183  -76.391785  -15.420312   \n",
       "26950 -92.002350 -27.655800   59.701550  66.647790   99.778190    3.915255   \n",
       "...          ...        ...         ...        ...         ...         ...   \n",
       "32542        NaN        NaN         NaN        NaN  -55.090855  -20.056421   \n",
       "28487        NaN -16.402885 -101.593440 -18.303556  -67.452095  -77.100980   \n",
       "15978 -79.426710 -41.448692  -55.587166 -58.608147  -52.567307  -45.682487   \n",
       "3200   11.005449   7.788031   70.938190 -12.596005   74.012405   81.418304   \n",
       "32231        NaN        NaN         NaN        NaN   11.579679    8.713304   \n",
       "\n",
       "       tSNE_GHI12  tSNE_GHI13  tSNE_GHI14  tSNE_GHIw_MEAN  tSNE_GHI14_within  \n",
       "35141         NaN   64.804930   61.886147       63.345539          -1.459392  \n",
       "30248  -39.898094    6.425584  -15.314579       17.199097         -32.513676  \n",
       "40563  -14.311967  -45.567310   27.308905       11.650547          15.658358  \n",
       "6954   -26.248714  -20.455055   -1.869290      -18.546666          16.677376  \n",
       "26950   19.751297    6.718432  -20.618025        3.479885         -24.097910  \n",
       "...           ...         ...         ...             ...                ...  \n",
       "32542  -20.870111   57.827724   65.454980        5.453063          60.001917  \n",
       "28487  -65.834885  -66.255850  -64.370340      -59.664254          -4.706086  \n",
       "15978   95.338830  -56.426400   94.130300      -21.118522         115.248822  \n",
       "3200    58.977135   70.751150   20.058348       41.707368         -21.649020  \n",
       "32231    7.781396   15.033057   68.194290       22.260345          45.933945  \n",
       "\n",
       "[1371 rows x 17 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_within"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
