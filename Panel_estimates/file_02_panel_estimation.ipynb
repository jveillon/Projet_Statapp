{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0307ef",
   "metadata": {},
   "source": [
    "Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747ca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # To standardize the data\n",
    "import cvxpy as cp\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb11d",
   "metadata": {},
   "source": [
    "Import of the HMLasso function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07cd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path \"C:/Users/Kilian/Desktop/ENSAE/STATAPP\" to run the cell\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Kilian/Desktop/ENSAE/STATAPP/Projet_Statapp/pretreatment')\n",
    "\n",
    "import file_04_HMLasso as hml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72f6ac",
   "metadata": {},
   "source": [
    "## Data downloading and separation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca111d",
   "metadata": {},
   "source": [
    "Dataset containing the types of each column from data_03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16325af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HHIDPN</td>\n",
       "      <td>Cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HHID</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PN</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  Type\n",
       "0  HHIDPN  Cont\n",
       "1    HHID  Char\n",
       "2      PN  Char"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_types = pd.read_csv(\"data_03_columns_types.csv\", index_col=0)\n",
    "columns_types.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbe86a",
   "metadata": {},
   "source": [
    "Downloading the data with social and genetic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015166c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\3952530018.py:1: DtypeWarning: Columns (2684) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data_03.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aec591",
   "metadata": {},
   "source": [
    "The column \"genetic_Section_A_or_E\" have mixed types, so we change its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965a817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = np.where(data['genetic_Section_A_or_E'] == 'E', 1, np.where(data['genetic_Section_A_or_E'] == 'A', 0, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f5d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genetic_Section_A_or_E\"] = temporary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b4e72",
   "metadata": {},
   "source": [
    "Now we add the health index created by t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6a30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_GHI = pd.read_csv(\"data_tSNE_GHI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fa4f3",
   "metadata": {},
   "source": [
    "We merge the t-SNE health index to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c121e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(tSNE_GHI, how ='left', on ='HHIDPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf31a8d",
   "metadata": {},
   "source": [
    "The final outcome to predict is tSNE_GHI14, so we only keep individuals who were interviewed during the last wave (14th wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ebbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bis = data[data['tSNE_GHI14'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ead7a",
   "metadata": {},
   "source": [
    "Number of individuals present in every waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7abd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSNE_GHI[~tSNE_GHI.isnull().any(axis=1)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04aaf5",
   "metadata": {},
   "source": [
    "We select the outcome tSNE_GHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39aa86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_bis[[\"HHIDPN\"]+[\"tSNE_GHI\" + str(i) for i in range (1,15)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563e1a",
   "metadata": {},
   "source": [
    "We drop the previous health index GHIw from the data, which won't be used as outcome.\n",
    "(list_columns_GHI contains the names of GHIw columns).\n",
    "\n",
    "We drop the outcome to create the matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_bis.drop([\"GHI\" + str(i) for i in range (1,15)], axis = 1)\n",
    "X.drop([\"tSNE_GHI\" + str(i) for i in range (1,15)], axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228abcf",
   "metadata": {},
   "source": [
    "Now we split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebb3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test, test_size=0.5, random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7c060",
   "metadata": {},
   "source": [
    "Smaller sets while coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51094fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test, nb_train, nb_valid = len(X_test.index)//10, len(X_train.index)//10, len(X_valid.index)//10\n",
    "X_test, Y_test = X_test.iloc[:nb_test], Y_test.iloc[:nb_test]\n",
    "X_train, Y_train = X_train.iloc[:nb_train], Y_train.iloc[:nb_train]\n",
    "X_valid, Y_valid = X_valid.iloc[:nb_valid], Y_valid.iloc[:nb_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ddedcf",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46496dbf",
   "metadata": {},
   "source": [
    "The objective here is to make a dataset where we observe if each variable exists at each wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cde8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_temporal_variables (X_train,add_tSNE_GHIw):   \n",
    "    temporal_variables = {}\n",
    "    waves_columns = [col for col in X_train.columns if \"genetic_\" not in col and col[1] in \"123456789\"]\n",
    "    for col in waves_columns:\n",
    "      char = col[0] # R or H\n",
    "      if col[2] in \"01234\":\n",
    "        wave = col[1:3]\n",
    "        suffix = col[3:]\n",
    "      else:\n",
    "        wave = col[1]\n",
    "        suffix = col[2:]\n",
    "      variable = char + 'w' + suffix\n",
    "\n",
    "      if variable not in temporal_variables.keys():\n",
    "        temporal_variables[variable] = np.zeros((14), dtype=bool)\n",
    "\n",
    "      temporal_variables[variable][int(wave)-1] = True\n",
    "\n",
    "    temporal_variables = pd.DataFrame(temporal_variables)\n",
    "\n",
    "    # We manually add \"tSNE_GHIw\":\n",
    "    if add_tSNE_GHIw:\n",
    "        temporal_variables[\"tSNE_GHIw\"] = np.ones((14), dtype=bool)\n",
    "        waves_columns += [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "        \n",
    "    return (temporal_variables,waves_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8869d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeless data\n",
    "def timeless_variables(X_train,waves_columns):\n",
    "    non_waves_columns = [col for col in X_train.columns if col not in waves_columns]\n",
    "    To_remove = [\"HHIDPN\",\"PN\",\"HHID\",\"RAHHIDPN\"]+[\"INW\"+str(i+1) for i in range (14)]\n",
    "    for x in To_remove:\n",
    "        non_waves_columns.remove(x)\n",
    "    return non_waves_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b3d71",
   "metadata": {},
   "source": [
    "We put the explaining variables by wave in a list of dataset Intemporal variables are put in each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3fbfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def list_wave(X_train, reduced):\n",
    "    (temporal_variables , waves_columns) = dataset_temporal_variables(X_train,True)\n",
    "    non_waves_columns = timeless_variables(X_train,waves_columns)\n",
    "    \n",
    "    #Reduce number of variables to code\n",
    "    if reduced:\n",
    "        temporal_variables_2 = temporal_variables.iloc[:,[i for i in range(1,15)]+[-i for i in range(1,5)]]\n",
    "        non_waves_columns_2 = random.choices(non_waves_columns,k=5)\n",
    "\n",
    "        liste = [] \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables_2.T[i].index[temporal_variables_2.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Add the intemporal variables only to the last wave, to avoid duplicated labels issues\n",
    "            if i == 13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns_2])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    #All the variables\n",
    "    else:\n",
    "        liste = []    # len = 14 \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables.T[i].index[temporal_variables.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Intemporal variables only to the last wave, to avoid duplicated labels issues\n",
    "            if i ==  13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    return (liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea850340",
   "metadata": {},
   "source": [
    "### Lasso selection\n",
    "\n",
    "We start to initialize with a first lasso on the first wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6c57b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Lasso(liste, Y_train, HMLasso, method, mu, limit):\n",
    "    \n",
    "    print(\"wave\",1)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    #Prepare data\n",
    "    X_train1 = liste[0].drop(\"HHIDPN\",axis=1)\n",
    "    Y_train1 = Y_train.iloc[:,1]\n",
    "    Y_train1.dropna(inplace =True)\n",
    "    Y_train1 = Y_train1.values\n",
    "    Y_train1 = (Y_train1 - np.mean(Y_train1))/np.std(Y_train1)\n",
    "  \n",
    "    #HMLasso\n",
    "    if HMLasso:\n",
    "        #Standardize X_train\n",
    "        X_train1 = scaler.fit_transform(X_train1)\n",
    "        \n",
    "        coefficients = apply_HMLasso(X_train1, Y_train1,mu)\n",
    "        \n",
    "        #Variables to keep\n",
    "        var_to_keep = coefficients > 10**(limit)\n",
    "    \n",
    "    #Common Lasso\n",
    "    else:\n",
    "        X_train1 = Na_imputation(X_train1, method)\n",
    "        \n",
    "        #Standardize X_train\n",
    "        X_train1 = scaler.fit_transform(X_train1)\n",
    "        \n",
    "        coefficients = apply_Lasso(X_train1, Y_train1, mu)\n",
    "        \n",
    "        #Variables top keep\n",
    "        var_to_keep = coefficients != 0\n",
    "\n",
    "    #Selection of variables\n",
    "    print(\"Variables kept :\", list(var_to_keep).count(1))\n",
    "    var_to_keep = np.insert(var_to_keep,0,True)\n",
    "    \n",
    "    entire_data = liste[0]\n",
    "    selected = entire_data[entire_data.columns[var_to_keep]]\n",
    "        \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "699ab071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_HMLasso(X, Y, mu):\n",
    "    \n",
    "    lasso = hml.HMLasso(mu)\n",
    "    lasso.fit(X, Y)\n",
    "    \n",
    "    coefficients = np.abs(lasso.beta_opt.copy())\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def apply_Lasso(X,Y, mu):\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=mu)\n",
    "    clf.fit(X, Y)\n",
    "    \n",
    "    coefficients = clf.coef_\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0f1c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_imputation(X, method):\n",
    "    if method == \"mean\":\n",
    "        return X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9394e1",
   "metadata": {},
   "source": [
    "function to impute missing data created when merging by mean but without touching Na values already there before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ab72eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_management(df1, df2, index):\n",
    "    \n",
    "    merged = df1.merge(df2, how='outer', on = index)\n",
    "    \n",
    "    df1_index = df1.set_index(index)\n",
    "    df2_index = df2.set_index(index)\n",
    "    \n",
    "    merged = merged.fillna(merged.mean())\n",
    "    merged = merged.set_index(index)\n",
    "    \n",
    "    df1_index = df1_index.fillna(\"NaN\")\n",
    "    merged.update(df1_index)\n",
    "    \n",
    "    df2_index = df2_index.fillna(\"NaN\")\n",
    "    merged.update(df2_index)\n",
    "    \n",
    "    merged = merged.replace(\"NaN\",np.nan)\n",
    "    \n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a387d69",
   "metadata": {},
   "source": [
    "Function to select variables by HMLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "539e1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso_selection(X_train, Y_train, HMLasso, method, mu, limit, reduced):\n",
    "    \n",
    "    #If a columns contains only Nan values, we drop it\n",
    "    empty_col = [col for col in X_train.columns if X_train[col].isnull().all()]\n",
    "    if empty_col != []:\n",
    "        X_train.drop(empty_col, axis=1, inplace=True)\n",
    "    \n",
    "    liste = list_wave(X_train, reduced)\n",
    "    \n",
    "    print(\"Lasso selection\")\n",
    "    \n",
    "    selected = initialize_Lasso(liste, Y_train, HMLasso, method, mu, limit)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    for i in range (1,14) :\n",
    "    \n",
    "        print(\"wave\",i+1)\n",
    "\n",
    "        var_to_select = Na_management(selected, liste[i], \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = Y_train.iloc[:,[0,i+1]]\n",
    "        X_Y_train = var_to_select.merge(Y_train_i, how = 'left', on = \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = X_Y_train[f\"tSNE_GHI{i+1}\"]\n",
    "        X_train_i = X_Y_train.drop([f\"tSNE_GHI{i+1}\",\"HHIDPN\"], axis =1)\n",
    "\n",
    "        Y_train_i = Y_train_i.fillna(Y_train_i.mean())\n",
    "        Y_train_i = Y_train_i.values\n",
    "        Y_train_i = (Y_train_i - np.mean(Y_train_i))/np.std(Y_train_i)\n",
    "\n",
    "        \n",
    "        #HMLasso\n",
    "        if HMLasso:\n",
    "            #Standardize X_train\n",
    "            X_train_i = scaler.fit_transform(X_train_i)\n",
    "        \n",
    "            coefficients = apply_HMLasso(X_train_i, Y_train_i, mu)\n",
    "        \n",
    "            #Variables to keep\n",
    "            var_to_keep = coefficients > 10**(limit)\n",
    "            \n",
    "        #Common Lasso\n",
    "        else:\n",
    "            X_train_i = Na_imputation(X_train_i, method)\n",
    "        \n",
    "            #Standardize X_train\n",
    "            X_train_i = scaler.fit_transform(X_train_i)\n",
    "\n",
    "            coefficients = apply_Lasso(X_train_i, Y_train_i, mu)\n",
    "\n",
    "            #Variables top keep\n",
    "            var_to_keep = coefficients != 0\n",
    "            \n",
    "        #Selection of variables\n",
    "        print(\"Variables kept :\", list(var_to_keep).count(1))\n",
    "        var_to_keep = np.insert(var_to_keep,0,True)\n",
    "\n",
    "        entire_data = var_to_select\n",
    "        selected = entire_data[entire_data.columns[var_to_keep]] \n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605b486",
   "metadata": {},
   "source": [
    "### Within estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cba15",
   "metadata": {},
   "source": [
    "There two types of missing values, the \"one-time\" missing values when someone didn't awnser a question during the interview or so and the missing values when someone wasn't interviewed at all during a wave.\n",
    "\n",
    "\n",
    "For the first type, we impute those missing values with the mean of the column (Nan).\n",
    "(Possibility to work on another imputation method).\n",
    "\n",
    "Then for the individuals who weren't interviewed during a wave, we replace the missing value with the temporal mean of the variable over time (NanNan)\n",
    "\n",
    "\n",
    "Finally, we compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2ba7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_data_within(selected):\n",
    "    \n",
    "    #For the \"one-time\" missing values imputation by mean\n",
    "    X_train_within = selected.fillna(selected.mean())\n",
    "    \n",
    "    ###For people who weren't interviewed\n",
    "    # We start by adding the INWw columns to know if the individual was interviewed during the wave w\n",
    "    X_train_within = X_train_within.merge(X_train[[\"HHIDPN\"]+[\"INW\"+str(i) for i in range(1,15)]], how =\"left\", on=\"HHIDPN\")  \n",
    "    \n",
    "    #We recover the missing values for people who weren't interviewed during the wave w\n",
    "    X_train_within = recover_missing(X_train_within)\n",
    "    \n",
    "    # Creation of the data set for within regression.\n",
    "    (X_train_within, temporal_variables_within) = data_set_within(X_train_within)\n",
    "    \n",
    "    #Still Nan values in intemporal variables\n",
    "    X_train_within = X_train_within.fillna(X_train_within.mean())\n",
    "    \n",
    "    return (X_train_within, temporal_variables_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feface31",
   "metadata": {},
   "source": [
    "This function return a dataset containing only variables concerned by the wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "464508b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "def get_wave(data, wave, non_temporal):\n",
    "  \"\"\"\n",
    "  This function returns a smaller dataset summarizing all data for the given wave.\n",
    "\n",
    "  Note that it also returns columns that are not relative to any wave (for instance, 'HHIDPN')\n",
    "  \"\"\"\n",
    "\n",
    "  assert wave in range(1, 15)\n",
    "\n",
    "  regex = re.compile(\"[0-9]+\")\n",
    "  if non_temporal:\n",
    "        wave_columns = [col for col in data.columns if (len(regex.findall(col)) == 0 or regex.findall(col)[0] == str(wave))]\n",
    "  else:\n",
    "        wave_columns = [col for col in data.columns if (regex.findall(col)[0] == str(wave))]\n",
    "  wave_data = data[wave_columns]\n",
    "\n",
    "  return wave_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf1ef0",
   "metadata": {},
   "source": [
    "Function to recover the missing values for people who weren't interviewed during the wave w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd5b9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_missing(X_train_within):\n",
    "\n",
    "    X_train_within_index = X_train_within.set_index(\"HHIDPN\")\n",
    "    wave_1 = get_wave(X_train_within_index,1, non_temporal =False)\n",
    "    wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
    "    wave_1[\"INW1\"].fillna(0)\n",
    "    Tempo = wave_1\n",
    "\n",
    "    for i in range(2,15):\n",
    "        if i == 14:\n",
    "            wave_i = get_wave(X_train_within_index, i, non_temporal =True)\n",
    "        else:\n",
    "            wave_i = get_wave(X_train_within_index, i, non_temporal =False)\n",
    "        wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
    "        wave_i[\"INWw\".replace('w', str(i))].fillna(0)\n",
    "        Tempo = Tempo.merge(wave_i, how= \"left\", on = \"HHIDPN\")\n",
    "\n",
    "    return Tempo.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de283800",
   "metadata": {},
   "source": [
    "Get a dataframe to know which variables are in X_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4691c",
   "metadata": {},
   "source": [
    "Function to compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)\n",
    "\n",
    "It creates columns containing the temporal mean of a temporal variables and then replaces the Nan values (when people weren't interviewed) by this mean. Finally it creates the dataset  X_vague_ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d188ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set_within (X_train_within):\n",
    "    \n",
    "    temporal_variables_within = dataset_temporal_variables(X_train_within, False)[0]\n",
    "    \n",
    "    X_within = X_train_within.copy()\n",
    "    \n",
    "    for col in temporal_variables_within.columns:\n",
    "        index_wave = temporal_variables_within.index[temporal_variables_within[col]==1].tolist()\n",
    "        names_waves = [col.replace('w', str(i+1)) for i in index_wave]\n",
    "        # (~X_within[names_waves].isna()).sum(axis=1) = number of non missing values\n",
    "        X_within[col+\"_MEAN\"] = X_within[names_waves].sum(axis=1)/(~X_within[names_waves].isna()).sum(axis=1)\n",
    "        for x in names_waves:\n",
    "            # Imputing the missing values by the temporal mean\n",
    "            new_col = X_within[x].fillna(X_within[col+\"_MEAN\"])\n",
    "            X_within[x] = new_col            \n",
    "            #Creating the new data for within regression X_vague\n",
    "            X_within[x] = X_within[x] - X_within[col+\"_MEAN\"]\n",
    "    return (X_within, temporal_variables_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488b723",
   "metadata": {},
   "source": [
    "Now we do the same thing to Y_train but no need to impute the Nan values since the only outcome is tSNE_GHI14 (no Nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae0a2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_outcome_within(Y_train):\n",
    "\n",
    "    Y_train_within = Y_train.copy()\n",
    "    \n",
    "    tSNE_GHI = [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "\n",
    "    Y_train_within[\"tSNE_GHIw_MEAN\"] = Y_train_within[tSNE_GHI].sum(axis=1)/(~Y_train_within[tSNE_GHI].isna()).sum(axis=1)\n",
    "    Y_train_within[\"tSNE_GHI14_within\"] = Y_train_within[\"tSNE_GHI14\"] - Y_train_within[\"tSNE_GHIw_MEAN\"]\n",
    "    \n",
    "    return Y_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95850bfa",
   "metadata": {},
   "source": [
    "We can now proceed to the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "506c2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_within(X_train_within, Y_train_within, temporal_variables_within):\n",
    "    \n",
    "    data_regression = X_train_within.merge(Y_train_within[[\"HHIDPN\",\"tSNE_GHI14_within\"]], on = \"HHIDPN\")\n",
    "    Y_regression = data_regression[\"tSNE_GHI14_within\"]\n",
    "    list_to_drop = [\"HHIDPN\",\"tSNE_GHI14_within\"]+[\"INW\"+str(i) for i in range(1,15)]+[col+\"_MEAN\" for col in temporal_variables_within.columns]\n",
    "    X_regression = data_regression.drop(list_to_drop,axis=1)\n",
    "    \n",
    "    return (X_regression, Y_regression)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2663f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def regression(X_regression,Y_regression):\n",
    "    \n",
    "    modeleReg=LinearRegression()\n",
    "\n",
    "    modeleReg.fit(X_regression,Y_regression) \n",
    "    \n",
    "    print(\"intercept :\" , modeleReg.intercept_)\n",
    "    print(\"coefficients :\", modeleReg.coef_)\n",
    "\n",
    "    #compute R²\n",
    "    print(\"R² :\", modeleReg.score(X_regression,Y_regression))\n",
    "    \n",
    "    return modeleReg.score(X_regression,Y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5c67266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Within_estimates(X, Y, HMLasso, method, mu, limit, reduced):\n",
    "    \n",
    "    selected = Lasso_selection(X, Y, HMLasso, method, mu, limit, reduced)\n",
    "        \n",
    "    (X_within, temporal_variables_within) = creation_data_within(selected)\n",
    "    Y_within = creation_outcome_within(Y_train)\n",
    "    \n",
    "    (X_regression, Y_regression) = get_X_Y_within(X_within, Y_within, temporal_variables_within)\n",
    "    \n",
    "    return regression(X_regression,Y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7573a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso selection\n",
      "wave 1\n",
      "Variables kept : 26\n",
      "wave 2\n",
      "Variables kept : 28\n",
      "wave 3\n",
      "Variables kept : 32\n",
      "wave 4\n",
      "Variables kept : 38\n",
      "wave 5\n",
      "Variables kept : 40\n",
      "wave 6\n",
      "Variables kept : 47\n",
      "wave 7\n",
      "Variables kept : 38\n",
      "wave 8\n",
      "Variables kept : 42\n",
      "wave 9\n",
      "Variables kept : 47\n",
      "wave 10\n",
      "Variables kept : 41\n",
      "wave 11\n",
      "Variables kept : 41\n",
      "wave 12\n",
      "Variables kept : 44\n",
      "wave 13\n",
      "Variables kept : 42\n",
      "wave 14\n",
      "Variables kept : 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_5948\\2199017923.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept : 5.892084151639099\n",
      "coefficients : [-2.87321339e+00  2.01235667e+00  1.14928695e+01 -6.93046491e+00\n",
      " -1.11540116e+00 -1.84983801e-01 -6.12222352e-01  1.50158938e+00\n",
      "  4.21884749e-14 -3.17523785e-14  2.06210985e-02 -1.86187899e-03\n",
      "  1.72869585e+00  8.07676249e-01 -9.96673024e-01  0.00000000e+00\n",
      " -1.41399534e+00  0.00000000e+00  1.57840725e+00 -3.37249815e-02\n",
      "  0.00000000e+00 -2.06210985e-02  0.00000000e+00  1.86187899e-03\n",
      "  0.00000000e+00  5.20176906e+00  9.19947260e-01  0.00000000e+00\n",
      " -2.33228923e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.41399534e+00  0.00000000e+00\n",
      " -1.57840725e+00 -1.14591445e+01  0.00000000e+00  0.00000000e+00]\n",
      "R² : 0.021940641267517824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.021940641267517824"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Within_estimates(X_train, Y_train, HMLasso = False, method=\"mean\", mu = 0.05, limit = -14, reduced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d457066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
