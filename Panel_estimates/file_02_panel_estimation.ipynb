{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0307ef",
   "metadata": {},
   "source": [
    "Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747ca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # To standardize the data\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb11d",
   "metadata": {},
   "source": [
    "Import of the HMLasso function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07cd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path \"C:/Users/Kilian/Desktop/ENSAE/STATAPP\" to run the cell\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Kilian/Desktop/ENSAE/STATAPP/Projet_Statapp/pretreatment')\n",
    "\n",
    "import file_04_HMLasso as hml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72f6ac",
   "metadata": {},
   "source": [
    "## Data downloading and separation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca111d",
   "metadata": {},
   "source": [
    "Dataset containing the types of each column from data_03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16325af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HHIDPN</td>\n",
       "      <td>Cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HHID</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PN</td>\n",
       "      <td>Char</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  Type\n",
       "0  HHIDPN  Cont\n",
       "1    HHID  Char\n",
       "2      PN  Char"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_types = pd.read_csv(\"data_03_columns_types.csv\", index_col=0)\n",
    "columns_types.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbe86a",
   "metadata": {},
   "source": [
    "Downloading the data with social and genetic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015166c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3952530018.py:1: DtypeWarning: Columns (2684) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data_03.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aec591",
   "metadata": {},
   "source": [
    "The column \"genetic_Section_A_or_E\" have mixed types, so we change its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965a817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = np.where(data['genetic_Section_A_or_E'] == 'E', 1, np.where(data['genetic_Section_A_or_E'] == 'A', 0, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f5d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genetic_Section_A_or_E\"] = temporary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b4e72",
   "metadata": {},
   "source": [
    "Now we add the health index created by t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6a30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_GHI = pd.read_csv(\"data_tSNE_GHI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fa4f3",
   "metadata": {},
   "source": [
    "We merge the t-SNE health index to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c121e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(tSNE_GHI, how ='left', on ='HHIDPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf31a8d",
   "metadata": {},
   "source": [
    "The final outcome to predict is tSNE_GHI14, so we only keep individuals who were interviewed during the last wave (14th wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ebbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bis = data[data['tSNE_GHI14'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ead7a",
   "metadata": {},
   "source": [
    "Number of individuals present in every waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7abd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSNE_GHI[~tSNE_GHI.isnull().any(axis=1)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04aaf5",
   "metadata": {},
   "source": [
    "We select the outcome tSNE_GHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_bis[[\"HHIDPN\"]+[\"tSNE_GHI\" + str(i) for i in range (1,15)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563e1a",
   "metadata": {},
   "source": [
    "We drop the previous health index GHIw from the data, which won't be used as outcome.\n",
    "(list_columns_GHI contains the names of GHIw columns).\n",
    "\n",
    "We drop the outcome to create the matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_bis.drop([\"GHI\" + str(i) for i in range (1,15)], axis = 1)\n",
    "X.drop([\"tSNE_GHI\" + str(i) for i in range (1,15)], axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228abcf",
   "metadata": {},
   "source": [
    "Now we split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bebb3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test, test_size=0.5, random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7c060",
   "metadata": {},
   "source": [
    "Smaller sets while coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51094fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test, nb_train, nb_valid = len(X_test.index)//10, len(X_train.index)//10, len(X_valid.index)//10\n",
    "X_test, Y_test = X_test.iloc[:nb_test], Y_test.iloc[:nb_test]\n",
    "X_train, Y_train = X_train.iloc[:nb_train], Y_train.iloc[:nb_train]\n",
    "X_valid, Y_valid = X_valid.iloc[:nb_valid], Y_valid.iloc[:nb_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ddedcf",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46496dbf",
   "metadata": {},
   "source": [
    "The objective here is to make a dataset where we observe if each variable exists at each wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cde8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_temporal_variables (X_train,add_tSNE_GHIw):   \n",
    "    temporal_variables = {}\n",
    "    waves_columns = [col for col in X_train.columns if \"genetic_\" not in col and col[1] in \"123456789\"]\n",
    "    for col in waves_columns:\n",
    "      char = col[0] # R or H\n",
    "      if col[2] in \"01234\":\n",
    "        wave = col[1:3]\n",
    "        suffix = col[3:]\n",
    "      else:\n",
    "        wave = col[1]\n",
    "        suffix = col[2:]\n",
    "      variable = char + 'w' + suffix\n",
    "\n",
    "      if variable not in temporal_variables.keys():\n",
    "        temporal_variables[variable] = np.zeros((14), dtype=bool)\n",
    "\n",
    "      temporal_variables[variable][int(wave)-1] = True\n",
    "\n",
    "    temporal_variables = pd.DataFrame(temporal_variables)\n",
    "\n",
    "    # We manually add \"tSNE_GHIw\":\n",
    "    if add_tSNE_GHIw:\n",
    "        temporal_variables[\"tSNE_GHIw\"] = np.ones((14), dtype=bool)\n",
    "        waves_columns += [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "        \n",
    "    return (temporal_variables,waves_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8869d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeless data\n",
    "def timeless_variables(X_train,waves_columns):\n",
    "    non_waves_columns = [col for col in X_train.columns if col not in waves_columns]\n",
    "    To_remove = [\"HHIDPN\",\"PN\",\"HHID\",\"RAHHIDPN\"]+[\"INW\"+str(i+1) for i in range (14)]\n",
    "    for x in To_remove:\n",
    "        non_waves_columns.remove(x)\n",
    "    return non_waves_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b3d71",
   "metadata": {},
   "source": [
    "We put the explaining variables by wave in a list of dataset Intemporal variables are put in each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fbfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def list_wave(X_train, reduced):\n",
    "    (temporal_variables , waves_columns) = dataset_temporal_variables(X_train,True)\n",
    "    non_waves_columns = timeless_variables(X_train,waves_columns)\n",
    "    \n",
    "    #Reduce number of variables to code\n",
    "    if reduced:\n",
    "        temporal_variables_2 = temporal_variables.iloc[:,[i for i in range(1,15)]+[-i for i in range(1,5)]]\n",
    "        non_waves_columns_2 = random.choices(non_waves_columns,k=5)\n",
    "\n",
    "        liste = [] \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables_2.T[i].index[temporal_variables_2.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Add the intemporal variables only to the last wave, to avoid duplicated labels issues\n",
    "            if i == 13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns_2])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    #All the variables\n",
    "    else:\n",
    "        liste = []    # len = 14 \n",
    "        for i in range(14):\n",
    "            columns_wave_i = [\"HHIDPN\"]+[col.replace('w', str(i+1)) for col in temporal_variables.T[i].index[temporal_variables.T[i]] if col != \"tSNE_GHIw\"]\n",
    "            #Intemporal variables only in the first wave, to avoid duplicated labels issues\n",
    "            if i ==  13:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i + non_waves_columns])\n",
    "            else:\n",
    "                liste.append(X_train.loc[X_train[\"INW\"+str(i+1)] == 1, columns_wave_i])\n",
    "                \n",
    "    return (liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea850340",
   "metadata": {},
   "source": [
    "### HMLasso selection\n",
    "\n",
    "We start to initialize with a first lasso on the first wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6c57b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_HMLasso(liste, Y_train, mu, limit):\n",
    "    \n",
    "    print(\"wave\",1)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    #Prepare data\n",
    "    X_train1 = liste[0].drop(\"HHIDPN\",axis=1)\n",
    "    Y_train1 = Y_train.iloc[:,1]\n",
    "    Y_train1.dropna(inplace =True)\n",
    "    Y_train1 = Y_train1.values\n",
    "    Y_train1 = (Y_train1 - np.mean(Y_train1))/np.std(Y_train1)\n",
    "    \n",
    "    #Standardize X_train\n",
    "    X_train1 = scaler.fit_transform(X_train1)\n",
    "    \n",
    "    #HMLasso\n",
    "    lasso = hml.HMLasso(mu)\n",
    "    lasso.fit(X_train1, Y_train1)\n",
    "    \n",
    "    #Selection of variables\n",
    "    coefficients = np.abs(lasso.beta_opt.copy())\n",
    "\n",
    "    var_to_keep = coefficients < 10**(limit)\n",
    "    var_to_keep = np.insert(var_to_keep,0,False)\n",
    "    \n",
    "    entire_data = liste[0]\n",
    "    selected = entire_data[entire_data.columns[~var_to_keep]]\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9394e1",
   "metadata": {},
   "source": [
    "function to impute missing data created when merging by mean but without touching Na values already there before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ab72eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Na_management(df1, df2, index):\n",
    "    \n",
    "    merged = df1.merge(df2, how='outer', on = index)\n",
    "    \n",
    "    df1_index = df1.set_index(index)\n",
    "    df2_index = df2.set_index(index)\n",
    "    \n",
    "    merged = merged.fillna(merged.mean())\n",
    "    merged = merged.set_index(index)\n",
    "    \n",
    "    df1_index = df1_index.fillna(\"NaN\")\n",
    "    merged.update(df1_index)\n",
    "    \n",
    "    df2_index = df2_index.fillna(\"NaN\")\n",
    "    merged.update(df2_index)\n",
    "    \n",
    "    merged = merged.replace(\"NaN\",np.nan)\n",
    "    \n",
    "    merged = merged.reset_index()\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a387d69",
   "metadata": {},
   "source": [
    "Function to select variables by HMLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "539e1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMLasso_selection(X_train, Y_train, mu, limit, reduced):\n",
    "    \n",
    "    liste = list_wave(X_train, reduced)\n",
    "    \n",
    "    print(\"HMLasso\")\n",
    "    \n",
    "    selected = initialize_HMLasso(liste, Y_train, mu, limit)\n",
    "    \n",
    "    scaler = StandardScaler()#(with_std=False)\n",
    "    hml.ERRORS_HANDLING = \"ignore\"\n",
    "    \n",
    "    for i in range (1,14) :\n",
    "    \n",
    "        print(\"wave\",i+1)\n",
    "\n",
    "        var_to_select = Na_management(selected, liste[i], \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = Y_train.iloc[:,[0,i+1]]\n",
    "        X_Y_train = var_to_select.merge(Y_train_i, how = 'left', on = \"HHIDPN\")\n",
    "\n",
    "        Y_train_i = X_Y_train[f\"tSNE_GHI{i+1}\"]\n",
    "        X_train_i = X_Y_train.drop([f\"tSNE_GHI{i+1}\",\"HHIDPN\"], axis =1)\n",
    "\n",
    "        Y_train_i = Y_train_i.fillna(Y_train_i.mean())\n",
    "        Y_train_i = Y_train_i.values\n",
    "        Y_train_i = (Y_train_i - np.mean(Y_train_i))/np.std(Y_train_i)\n",
    "\n",
    "        X_train_i = scaler.fit_transform(X_train_i)\n",
    "\n",
    "        lasso = hml.HMLasso(mu)\n",
    "        lasso.fit(X_train_i, Y_train_i)\n",
    "\n",
    "        coefficients = np.abs(lasso.beta_opt.copy())\n",
    "\n",
    "        var_to_keep = coefficients < 10**(limit)\n",
    "        var_to_keep = np.insert(var_to_keep,0,False)\n",
    "\n",
    "        entire_data = var_to_select\n",
    "        selected = entire_data[entire_data.columns[~var_to_keep]] \n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697719a9",
   "metadata": {},
   "source": [
    "### Standard Lasso selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605b486",
   "metadata": {},
   "source": [
    "### Within estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cba15",
   "metadata": {},
   "source": [
    "There two types of missing values, the \"one-time\" missing values when someone didn't awnser a question during the interview or so and the missing values when someone wasn't interviewed at all during a wave.\n",
    "\n",
    "\n",
    "For the first type, we impute those missing values with the mean of the column (Nan).\n",
    "(Possibility to work on another imputation method).\n",
    "\n",
    "Then for the individuals who weren't interviewed during a wave, we replace the missing value with the temporal mean of the variable over time (NanNan)\n",
    "\n",
    "\n",
    "Finally, we compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2ba7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_data_within(selected):\n",
    "    \n",
    "    #For the \"one-time\" missing values imputation by mean\n",
    "    X_train_within = selected.fillna(selected.mean())\n",
    "    \n",
    "    ###For people who weren't interviewed\n",
    "    # We start by adding the INWw columns to know if the individual was interviewed during the wave w\n",
    "    X_train_within = X_train_within.merge(X_train[[\"HHIDPN\"]+[\"INW\"+str(i) for i in range(1,15)]], how =\"left\", on=\"HHIDPN\")  \n",
    "    \n",
    "    #We recover the missing values for people who weren't interviewed during the wave w\n",
    "    X_train_within = recover_missing(X_train_within)\n",
    "    \n",
    "    # Creation of the data set for within regression.\n",
    "    (X_train_within, temporal_variables_within) = data_set_within(X_train_within)\n",
    "    \n",
    "    #Still Nan values in intemporal variables\n",
    "    X_train_within = X_train_within.fillna(X_train_within.mean())\n",
    "    \n",
    "    return (X_train_within, temporal_variables_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feface31",
   "metadata": {},
   "source": [
    "This function return a dataset containing only variables concerned by the wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "464508b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "def get_wave(data, wave):\n",
    "  \"\"\"\n",
    "  This function returns a smaller dataset summarizing all data for the given wave.\n",
    "\n",
    "  Note that it also returns columns that are not relative to any wave (for instance, 'HHIDPN')\n",
    "  \"\"\"\n",
    "\n",
    "  assert wave in range(1, 15)\n",
    "\n",
    "  regex = re.compile(\"[0-9]+\")\n",
    "  wave_columns = [col for col in data.columns if (len(regex.findall(col)) == 0 or regex.findall(col)[0] == str(wave))]\n",
    "  wave_data = data[wave_columns]\n",
    "\n",
    "  return wave_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf1ef0",
   "metadata": {},
   "source": [
    "Function to recover the missing values for people who weren't interviewed during the wave w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dd5b9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_missing(X_train_within):\n",
    "\n",
    "    X_train_within_index = X_train_within.set_index(\"HHIDPN\")\n",
    "    wave_1 = get_wave(X_train_within_index,1)\n",
    "    wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
    "    wave_1[\"INW1\"].fillna(0)\n",
    "    Tempo = wave_1\n",
    "\n",
    "    for i in range(2,15):\n",
    "        wave_i = get_wave(X_train_within_index, i)\n",
    "        wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
    "        wave_i[\"INWw\".replace('w', str(i))].fillna(0)\n",
    "        Tempo = Tempo.merge(wave_i, how= \"left\", on = \"HHIDPN\")\n",
    "\n",
    "    return Tempo.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de283800",
   "metadata": {},
   "source": [
    "Get a dataframe to know which variables are in X_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4691c",
   "metadata": {},
   "source": [
    "Function to compute (A faire en latex) X_vague_ti = X_ti - temporal_mean(X_ti)\n",
    "\n",
    "It creates columns containing the temporal mean of a temporal variables and then replaces the Nan values (when people weren't interviewed) by this mean. Finally it creates the dataset  X_vague_ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d188ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set_within (X_train_within):\n",
    "    \n",
    "    temporal_variables_within = dataset_temporal_variables(X_train_within, False)[0]\n",
    "    \n",
    "    X_within = X_train_within.copy()\n",
    "    \n",
    "    for col in temporal_variables_within.columns:\n",
    "        index_wave = temporal_variables_within.index[temporal_variables_within[col]==1].tolist()\n",
    "        names_waves = [col.replace('w', str(i+1)) for i in index_wave]\n",
    "        # (~X_within[names_waves].isna()).sum(axis=1) = number of non missing values\n",
    "        X_within[col+\"_MEAN\"] = X_within[names_waves].sum(axis=1)/(~X_within[names_waves].isna()).sum(axis=1)\n",
    "        for x in names_waves:\n",
    "            # Imputing the missing values by the temporal mean\n",
    "            new_col = X_within[x].fillna(X_within[col+\"_MEAN\"])\n",
    "            X_within[x] = new_col            \n",
    "            #Creating the new data for within regression X_vague\n",
    "            X_within[x] = X_within[x] - X_within[col+\"_MEAN\"]\n",
    "    return (X_within, temporal_variables_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488b723",
   "metadata": {},
   "source": [
    "Now we do the same thing to Y_train but no need to impute the Nan values since the only outcome is tSNE_GHI14 (no Nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f953063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_outcome_within(Y_train):\n",
    "\n",
    "    Y_train_within = Y_train.copy()\n",
    "    \n",
    "    tSNE_GHI = [f\"tSNE_GHI{w}\" for w in range(1,15)]\n",
    "\n",
    "    Y_train_within[\"tSNE_GHIw_MEAN\"] = Y_train_within[tSNE_GHI].sum(axis=1)/(~Y_train_within[tSNE_GHI].isna()).sum(axis=1)\n",
    "    Y_train_within[\"tSNE_GHI14_within\"] = Y_train_within[\"tSNE_GHI14\"] - Y_train_within[\"tSNE_GHIw_MEAN\"]\n",
    "    \n",
    "    return Y_train_within"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95850bfa",
   "metadata": {},
   "source": [
    "We can now proceed to the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e6622e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_within(X_train_within, Y_train_within, temporal_variables_within):\n",
    "    \n",
    "    data_regression = X_train_within.merge(Y_train_within[[\"HHIDPN\",\"tSNE_GHI14_within\"]], on = \"HHIDPN\")\n",
    "    Y_regression = data_regression[\"tSNE_GHI14_within\"]\n",
    "    list_to_drop = [\"HHIDPN\",\"tSNE_GHI14_within\"]+[\"INW\"+str(i) for i in range(1,15)]+[col+\"_MEAN\" for col in temporal_variables_within.columns]\n",
    "    X_regression = data_regression.drop(list_to_drop,axis=1)\n",
    "    \n",
    "    return (X_regression, Y_regression)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "32763c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def regression(X_regression,Y_regression):\n",
    "    \n",
    "    modeleReg=LinearRegression()\n",
    "\n",
    "    modeleReg.fit(X_regression,Y_regression) \n",
    "    \n",
    "    print(\"intercept :\" , modeleReg.intercept_)\n",
    "    print(\"coefficients :\", modeleReg.coef_)\n",
    "\n",
    "    #compute R²\n",
    "    print(\"R² :\", modeleReg.score(X_regression,Y_regression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51975a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Within_estimates(X, Y, HMLasso, mu, limit, reduced):\n",
    "    \n",
    "    if HMLasso:\n",
    "        selected = HMLasso_selection(X, Y, mu, limit, reduced)\n",
    "    else:\n",
    "        selected = Lasso_selection(X, Y, mu, limit, reduced)\n",
    "        \n",
    "    (X_within, temporal_variables_within) = creation_data_within(selected)\n",
    "    Y_within = creation_outcome_within(Y_train)\n",
    "    \n",
    "    (X_regression, Y_regression) = get_X_Y_within(X_within, Y_within, temporal_variables_within)\n",
    "    \n",
    "    regression(X_regression,Y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74941205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMLasso\n",
      "wave 1\n",
      "wave 2\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -4.5464001906118986e-06. Error handled by adding 4.5464001906118986e-06 to each eigenvalue.\n",
      "wave 3\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -6.804841462278468e-06. Error handled by adding 6.804841462278468e-06 to each eigenvalue.\n",
      "wave 4\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -5.4432311370396063e-05. Error handled by adding 5.4432311370396063e-05 to each eigenvalue.\n",
      "wave 5\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -3.871227124529014e-06. Error handled by adding 3.871227124529014e-06 to each eigenvalue.\n",
      "wave 6\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -4.204490209059371e-06. Error handled by adding 4.204490209059371e-06 to each eigenvalue.\n",
      "wave 7\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -3.5785352014151535e-05. Error handled by adding 3.5785352014151535e-05 to each eigenvalue.\n",
      "wave 8\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -6.457500527506386e-05. Error handled by adding 6.457500527506386e-05 to each eigenvalue.\n",
      "wave 9\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -6.67358069924696e-05. Error handled by adding 6.67358069924696e-05 to each eigenvalue.\n",
      "wave 10\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -8.036412941876353e-05. Error handled by adding 8.036412941876353e-05 to each eigenvalue.\n",
      "wave 11\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -7.24168523453797e-05. Error handled by adding 7.24168523453797e-05 to each eigenvalue.\n",
      "wave 12\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -7.07905950807202e-05. Error handled by adding 7.07905950807202e-05 to each eigenvalue.\n",
      "wave 13\n",
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -0.0001642894047502537. Error handled by adding 0.0001642894047502537 to each eigenvalue.\n",
      "wave 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:985: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:990: RuntimeWarning: invalid value encountered in true_divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\Kilian\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:1020: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Sigma_opt is not PSD, its minimum eigenvalue is -6.0465715525218893e-05. Error handled by adding 6.0465715525218893e-05 to each eigenvalue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_1.loc[wave_1[\"INW1\"] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n",
      "C:\\Users\\Kilian\\AppData\\Local\\Temp\\ipykernel_12364\\3189160166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wave_i.loc[wave_i[\"INWw\".replace('w', str(i))] == 0] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept : 0.37874600754707366\n",
      "coefficients : [ 5.50589417e+02 -4.68667657e+10 -2.31246148e+11 -4.20900983e+10\n",
      " -1.59145777e+09  2.66696635e+08 -1.34253368e+10 -1.23099978e+10\n",
      " -1.35273251e+10 -9.38612180e+09  1.26049171e+08 -1.05024827e+10\n",
      " -2.10543624e+11  5.48133474e+02 -4.68667657e+10  1.78896438e+11\n",
      "  5.82517955e+10 -1.59145798e+09  2.66696555e+08 -1.34253368e+10\n",
      " -1.23099979e+10 -1.35273251e+10 -9.38612186e+09  1.26049162e+08\n",
      " -1.05024827e+10 -2.10543624e+11  5.54649180e+02 -4.68667657e+10\n",
      "  2.71101716e+09 -3.62606555e+10 -1.59145787e+09 -1.34253368e+10\n",
      " -1.23099978e+10 -1.35273250e+10 -9.38612184e+09  1.26049195e+08\n",
      " -1.05024827e+10 -2.10543624e+11 -8.04666202e+07  5.52440857e+02\n",
      " -4.68667657e+10  1.87710800e+09  4.77064335e+10 -1.59145805e+09\n",
      "  2.66696469e+08 -1.34253368e+10 -1.23099978e+10 -1.35273250e+10\n",
      " -9.38612191e+09  1.26049154e+08 -1.05024827e+10 -2.10543624e+11\n",
      " -2.22820881e+07 -8.04666198e+07  4.93319143e+00  5.51129663e+02\n",
      " -4.68667657e+10  2.92582581e+09  8.41120766e+09 -1.59145799e+09\n",
      "  2.66696509e+08 -1.34253368e+10 -1.23099978e+10 -1.35273250e+10\n",
      " -9.38612191e+09  1.26049154e+08 -1.05024827e+10 -2.10543624e+11\n",
      " -2.22822021e+07 -8.04665704e+07  5.52236401e+02 -4.68667657e+10\n",
      "  2.88302602e+09  1.51115345e+10 -1.34253368e+10 -1.23099979e+10\n",
      " -1.35273250e+10 -9.38612187e+09  1.26049203e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93921769e+07 -2.22820182e+07 -8.04666295e+07\n",
      "  5.51711390e+02 -4.68667657e+10  1.50218913e+10 -1.59145799e+09\n",
      " -1.34253368e+10 -1.23099978e+10 -1.35273250e+10 -9.38612185e+09\n",
      "  1.26049167e+08 -1.05024827e+10 -2.10543624e+11 -2.22819669e+07\n",
      " -8.04665991e+07  5.51925430e+02 -4.68667657e+10  1.50384265e+10\n",
      " -1.59145788e+09  2.66696523e+08 -1.34253368e+10 -1.23099979e+10\n",
      " -1.35273250e+10 -9.38612188e+09  1.26049159e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93919969e+07 -2.22822815e+07 -8.04665901e+07\n",
      "  5.51546566e+02 -4.68667657e+10 -3.11494635e+09 -1.59145797e+09\n",
      "  2.66696550e+08 -1.34253368e+10 -1.23099979e+10 -1.35273250e+10\n",
      " -9.38612187e+09  1.26049194e+08 -1.05024827e+10 -2.10543624e+11\n",
      "  1.93922363e+07 -2.22820654e+07 -8.04666196e+07 -1.05215817e+01\n",
      "  5.46864150e+02 -4.68667657e+10  2.87939932e+09  1.11280177e+10\n",
      " -1.59145792e+09  2.66696508e+08 -1.34253368e+10 -1.23099979e+10\n",
      " -1.35273250e+10 -9.38612188e+09  1.26049168e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93920690e+07 -2.22820862e+07 -8.04666101e+07\n",
      "  5.57148670e+02 -4.68667657e+10  2.87686808e+09 -3.85125239e+09\n",
      " -1.59145799e+09  2.66696441e+08 -1.34253368e+10 -1.23099978e+10\n",
      " -1.35273250e+10 -9.38612190e+09  1.26049168e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93920679e+07 -2.22821334e+07 -8.04666313e+07\n",
      "  5.50351269e+02 -4.68667657e+10 -7.31891658e+09  6.38926861e+09\n",
      " -1.59145787e+09  2.66696564e+08 -1.34253368e+10 -1.23099979e+10\n",
      " -1.35273250e+10 -9.38612184e+09  1.26049167e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93921071e+07 -2.22820713e+07 -8.04666118e+07\n",
      "  5.50940657e+02 -4.68667657e+10 -7.32886462e+09  5.17198763e+08\n",
      " -1.59145794e+09  2.66696488e+08 -1.34253368e+10 -1.23099978e+10\n",
      " -1.35273250e+10 -9.38612187e+09  1.26049177e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93921177e+07 -2.22820924e+07 -8.04665969e+07\n",
      "  5.50892181e+02 -4.68667657e+10 -7.32886462e+09  8.59349054e+09\n",
      " -1.59145791e+09  2.66696482e+08 -1.34253368e+10 -1.23099979e+10\n",
      " -1.35273250e+10 -9.38612190e+09  1.26049169e+08 -1.05024827e+10\n",
      " -2.10543624e+11  1.93921071e+07 -2.22820847e+07 -8.04665892e+07]\n",
      "R² : 0.22421776641436697\n"
     ]
    }
   ],
   "source": [
    "Within_estimates(X_train, Y_train, HMLasso = True, mu = 250, limit = -14, reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958594c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
